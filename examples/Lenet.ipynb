{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows an example of using the Multi output module on a Lenet5 for uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import log_loss, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchmetrics import CalibrationError\n",
    "\n",
    "from models.lenet import LeNet5\n",
    "from multi_output_module.multi_output_module import Multi_output_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = './data'\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_train_dataset = datasets.MNIST(root=data_path,\n",
    "                                     train=True,\n",
    "                                     transform=transforms.ToTensor(),\n",
    "                                     download=True)\n",
    "\n",
    "mnist_test_dataset = datasets.MNIST(root=data_path,\n",
    "                                    train=False,\n",
    "                                    transform=transforms.ToTensor())\n",
    "\n",
    "mnist_ood_dataset = datasets.FashionMNIST(root=data_path,\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "\n",
    "train_size = int(0.8 * len(mnist_train_dataset))\n",
    "val_size = len(mnist_train_dataset) - train_size\n",
    "mnist_train_dataset, mnist_val_dataset = random_split(mnist_train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(mnist_train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(mnist_val_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_dataset, batch_size=32, shuffle=True)\n",
    "fashion_ood_loader = DataLoader(mnist_ood_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the transformation for CIFAR-100 dataset\n",
    "cifar100_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert CIFAR-100 images to grayscale\n",
    "    transforms.Resize((28, 28)),                 # Resize to match MNIST image size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "cifar100_dataset = datasets.CIFAR100(root=data_path, train=False, transform=cifar100_transform, download=True)\n",
    "cifar100_ood_loader = DataLoader(cifar100_dataset, batch_size=32, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LeNet5()\n",
    "\n",
    "# Load the state dictionary from the .pth file\n",
    "base_model.load_state_dict(torch.load('./models/saved_models/mnist/mnist_lenet5.pth'))\n",
    "\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0517, Accuracy: 9849/10000 (98.49%)\n",
      "\n",
      "Reloaded model test accuracy: 98.49%\n"
     ]
    }
   ],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test(base_model, device, test_loader)\n",
    "print(f'Reloaded model test accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 3\n",
    "batch_size = 16\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_output_module(\n",
      "  (activation): ReLU()\n",
      "  (base_model): LeNet5(\n",
      "    (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      "  (last_layer): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (input_heads): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      "  (shared_layers): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (output_layers): ModuleList(\n",
      "    (0-2): 3 x Linear(in_features=30, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model.eval()\n",
    "    \n",
    "module = Multi_output_module(num_heads, base_model, device).to(device)\n",
    "print(module)\n",
    "\n",
    "#optimizer = optim.Adam(module.parameters(), lr=0.0008, weight_decay= 0.0005)\n",
    "\n",
    "optimizer = optim.Adam(module.parameters(), lr=0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_train_dataset,\n",
    "    batch_size = num_heads* batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 48836\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in module.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0%)]\tLoss: 4.570747\n",
      "Train Epoch: 0 [4800/48000 (10%)]\tLoss: 3.690549\n",
      "Train Epoch: 0 [9600/48000 (20%)]\tLoss: 2.701743\n",
      "Train Epoch: 0 [14400/48000 (30%)]\tLoss: 2.326173\n",
      "Train Epoch: 0 [19200/48000 (40%)]\tLoss: 2.283152\n",
      "Train Epoch: 0 [24000/48000 (50%)]\tLoss: 2.171135\n",
      "Train Epoch: 0 [28800/48000 (60%)]\tLoss: 1.769102\n",
      "Train Epoch: 0 [33600/48000 (70%)]\tLoss: 1.934822\n",
      "Train Epoch: 0 [38400/48000 (80%)]\tLoss: 1.685667\n",
      "Train Epoch: 0 [43200/48000 (90%)]\tLoss: 1.832784\n",
      "Epoch 0, Val Loss: 0.003881, Accuracy: 11884/12000 (99.03%)\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 1.531632\n",
      "Train Epoch: 1 [4800/48000 (10%)]\tLoss: 1.492050\n",
      "Train Epoch: 1 [9600/48000 (20%)]\tLoss: 1.575234\n",
      "Train Epoch: 1 [14400/48000 (30%)]\tLoss: 1.556983\n",
      "Train Epoch: 1 [19200/48000 (40%)]\tLoss: 1.241067\n",
      "Train Epoch: 1 [24000/48000 (50%)]\tLoss: 1.196264\n",
      "Train Epoch: 1 [28800/48000 (60%)]\tLoss: 1.100542\n",
      "Train Epoch: 1 [33600/48000 (70%)]\tLoss: 0.843922\n",
      "Train Epoch: 1 [38400/48000 (80%)]\tLoss: 1.116934\n",
      "Train Epoch: 1 [43200/48000 (90%)]\tLoss: 1.050974\n",
      "Epoch 1, Val Loss: 0.002487, Accuracy: 11858/12000 (98.82%)\n",
      "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.942734\n",
      "Train Epoch: 2 [4800/48000 (10%)]\tLoss: 0.820214\n",
      "Train Epoch: 2 [9600/48000 (20%)]\tLoss: 0.742795\n",
      "Train Epoch: 2 [14400/48000 (30%)]\tLoss: 0.667307\n",
      "Train Epoch: 2 [19200/48000 (40%)]\tLoss: 0.815908\n",
      "Train Epoch: 2 [24000/48000 (50%)]\tLoss: 0.708326\n",
      "Train Epoch: 2 [28800/48000 (60%)]\tLoss: 0.423190\n",
      "Train Epoch: 2 [33600/48000 (70%)]\tLoss: 0.677070\n",
      "Train Epoch: 2 [38400/48000 (80%)]\tLoss: 0.529648\n",
      "Train Epoch: 2 [43200/48000 (90%)]\tLoss: 0.389621\n",
      "Epoch 2, Val Loss: 0.001834, Accuracy: 11855/12000 (98.79%)\n",
      "Train Epoch: 3 [0/48000 (0%)]\tLoss: 0.505243\n",
      "Train Epoch: 3 [4800/48000 (10%)]\tLoss: 0.485518\n",
      "Train Epoch: 3 [9600/48000 (20%)]\tLoss: 0.373279\n",
      "Train Epoch: 3 [14400/48000 (30%)]\tLoss: 0.548012\n",
      "Train Epoch: 3 [19200/48000 (40%)]\tLoss: 0.387782\n",
      "Train Epoch: 3 [24000/48000 (50%)]\tLoss: 0.328105\n",
      "Train Epoch: 3 [28800/48000 (60%)]\tLoss: 0.379895\n",
      "Train Epoch: 3 [33600/48000 (70%)]\tLoss: 0.310791\n",
      "Train Epoch: 3 [38400/48000 (80%)]\tLoss: 0.284753\n",
      "Train Epoch: 3 [43200/48000 (90%)]\tLoss: 0.387786\n",
      "Epoch 3, Val Loss: 0.001581, Accuracy: 11858/12000 (98.82%)\n",
      "Train Epoch: 4 [0/48000 (0%)]\tLoss: 0.396852\n",
      "Train Epoch: 4 [4800/48000 (10%)]\tLoss: 0.422172\n",
      "Train Epoch: 4 [9600/48000 (20%)]\tLoss: 0.520761\n",
      "Train Epoch: 4 [14400/48000 (30%)]\tLoss: 0.282923\n",
      "Train Epoch: 4 [19200/48000 (40%)]\tLoss: 0.185987\n",
      "Train Epoch: 4 [24000/48000 (50%)]\tLoss: 0.298144\n",
      "Train Epoch: 4 [28800/48000 (60%)]\tLoss: 0.231719\n",
      "Train Epoch: 4 [33600/48000 (70%)]\tLoss: 0.271581\n",
      "Train Epoch: 4 [38400/48000 (80%)]\tLoss: 0.172458\n",
      "Train Epoch: 4 [43200/48000 (90%)]\tLoss: 0.300973\n",
      "Epoch 4, Val Loss: 0.001431, Accuracy: 11857/12000 (98.81%)\n",
      "Train Epoch: 5 [0/48000 (0%)]\tLoss: 0.350180\n",
      "Train Epoch: 5 [4800/48000 (10%)]\tLoss: 0.144139\n",
      "Train Epoch: 5 [9600/48000 (20%)]\tLoss: 0.113648\n",
      "Train Epoch: 5 [14400/48000 (30%)]\tLoss: 0.199157\n",
      "Train Epoch: 5 [19200/48000 (40%)]\tLoss: 0.101607\n",
      "Train Epoch: 5 [24000/48000 (50%)]\tLoss: 0.110032\n",
      "Train Epoch: 5 [28800/48000 (60%)]\tLoss: 0.105927\n",
      "Train Epoch: 5 [33600/48000 (70%)]\tLoss: 0.181251\n",
      "Train Epoch: 5 [38400/48000 (80%)]\tLoss: 0.304747\n",
      "Train Epoch: 5 [43200/48000 (90%)]\tLoss: 0.063950\n",
      "Epoch 5, Val Loss: 0.001319, Accuracy: 11866/12000 (98.88%)\n",
      "Train Epoch: 6 [0/48000 (0%)]\tLoss: 0.222749\n",
      "Train Epoch: 6 [4800/48000 (10%)]\tLoss: 0.246497\n",
      "Train Epoch: 6 [9600/48000 (20%)]\tLoss: 0.282736\n",
      "Train Epoch: 6 [14400/48000 (30%)]\tLoss: 0.157795\n",
      "Train Epoch: 6 [19200/48000 (40%)]\tLoss: 0.127872\n",
      "Train Epoch: 6 [24000/48000 (50%)]\tLoss: 0.345493\n",
      "Train Epoch: 6 [28800/48000 (60%)]\tLoss: 0.303764\n",
      "Train Epoch: 6 [33600/48000 (70%)]\tLoss: 0.120022\n",
      "Train Epoch: 6 [38400/48000 (80%)]\tLoss: 0.232413\n",
      "Train Epoch: 6 [43200/48000 (90%)]\tLoss: 0.081737\n",
      "Epoch 6, Val Loss: 0.001246, Accuracy: 11864/12000 (98.87%)\n",
      "Train Epoch: 7 [0/48000 (0%)]\tLoss: 0.140946\n",
      "Train Epoch: 7 [4800/48000 (10%)]\tLoss: 0.170161\n",
      "Train Epoch: 7 [9600/48000 (20%)]\tLoss: 0.182682\n",
      "Train Epoch: 7 [14400/48000 (30%)]\tLoss: 0.147083\n",
      "Train Epoch: 7 [19200/48000 (40%)]\tLoss: 0.160238\n",
      "Train Epoch: 7 [24000/48000 (50%)]\tLoss: 0.140622\n",
      "Train Epoch: 7 [28800/48000 (60%)]\tLoss: 0.199556\n",
      "Train Epoch: 7 [33600/48000 (70%)]\tLoss: 0.167295\n",
      "Train Epoch: 7 [38400/48000 (80%)]\tLoss: 0.206502\n",
      "Train Epoch: 7 [43200/48000 (90%)]\tLoss: 0.123838\n",
      "Epoch 7, Val Loss: 0.001235, Accuracy: 11855/12000 (98.79%)\n",
      "Train Epoch: 8 [0/48000 (0%)]\tLoss: 0.090162\n",
      "Train Epoch: 8 [4800/48000 (10%)]\tLoss: 0.076353\n",
      "Train Epoch: 8 [9600/48000 (20%)]\tLoss: 0.170655\n",
      "Train Epoch: 8 [14400/48000 (30%)]\tLoss: 0.164035\n",
      "Train Epoch: 8 [19200/48000 (40%)]\tLoss: 0.083777\n",
      "Train Epoch: 8 [24000/48000 (50%)]\tLoss: 0.137231\n",
      "Train Epoch: 8 [28800/48000 (60%)]\tLoss: 0.094063\n",
      "Train Epoch: 8 [33600/48000 (70%)]\tLoss: 0.159944\n",
      "Train Epoch: 8 [38400/48000 (80%)]\tLoss: 0.117473\n",
      "Train Epoch: 8 [43200/48000 (90%)]\tLoss: 0.130448\n",
      "Epoch 8, Val Loss: 0.001174, Accuracy: 11866/12000 (98.88%)\n",
      "Train Epoch: 9 [0/48000 (0%)]\tLoss: 0.047598\n",
      "Train Epoch: 9 [4800/48000 (10%)]\tLoss: 0.191159\n",
      "Train Epoch: 9 [9600/48000 (20%)]\tLoss: 0.096345\n",
      "Train Epoch: 9 [14400/48000 (30%)]\tLoss: 0.057978\n",
      "Train Epoch: 9 [19200/48000 (40%)]\tLoss: 0.166810\n",
      "Train Epoch: 9 [24000/48000 (50%)]\tLoss: 0.039495\n",
      "Train Epoch: 9 [28800/48000 (60%)]\tLoss: 0.115426\n",
      "Train Epoch: 9 [33600/48000 (70%)]\tLoss: 0.163496\n",
      "Train Epoch: 9 [38400/48000 (80%)]\tLoss: 0.154592\n",
      "Train Epoch: 9 [43200/48000 (90%)]\tLoss: 0.147553\n",
      "Epoch 9, Val Loss: 0.001124, Accuracy: 11876/12000 (98.97%)\n",
      "Train Epoch: 10 [0/48000 (0%)]\tLoss: 0.027554\n",
      "Train Epoch: 10 [4800/48000 (10%)]\tLoss: 0.121355\n",
      "Train Epoch: 10 [9600/48000 (20%)]\tLoss: 0.214884\n",
      "Train Epoch: 10 [14400/48000 (30%)]\tLoss: 0.060191\n",
      "Train Epoch: 10 [19200/48000 (40%)]\tLoss: 0.224035\n",
      "Train Epoch: 10 [24000/48000 (50%)]\tLoss: 0.102108\n",
      "Train Epoch: 10 [28800/48000 (60%)]\tLoss: 0.190447\n",
      "Train Epoch: 10 [33600/48000 (70%)]\tLoss: 0.149814\n",
      "Train Epoch: 10 [38400/48000 (80%)]\tLoss: 0.011830\n",
      "Train Epoch: 10 [43200/48000 (90%)]\tLoss: 0.112900\n",
      "Epoch 10, Val Loss: 0.001090, Accuracy: 11876/12000 (98.97%)\n",
      "Train Epoch: 11 [0/48000 (0%)]\tLoss: 0.107775\n",
      "Train Epoch: 11 [4800/48000 (10%)]\tLoss: 0.069295\n",
      "Train Epoch: 11 [9600/48000 (20%)]\tLoss: 0.243977\n",
      "Train Epoch: 11 [14400/48000 (30%)]\tLoss: 0.059068\n",
      "Train Epoch: 11 [19200/48000 (40%)]\tLoss: 0.054356\n",
      "Train Epoch: 11 [24000/48000 (50%)]\tLoss: 0.083924\n",
      "Train Epoch: 11 [28800/48000 (60%)]\tLoss: 0.054826\n",
      "Train Epoch: 11 [33600/48000 (70%)]\tLoss: 0.195641\n",
      "Train Epoch: 11 [38400/48000 (80%)]\tLoss: 0.050065\n",
      "Train Epoch: 11 [43200/48000 (90%)]\tLoss: 0.264016\n",
      "Epoch 11, Val Loss: 0.001061, Accuracy: 11874/12000 (98.95%)\n",
      "Train Epoch: 12 [0/48000 (0%)]\tLoss: 0.015550\n",
      "Train Epoch: 12 [4800/48000 (10%)]\tLoss: 0.064714\n",
      "Train Epoch: 12 [9600/48000 (20%)]\tLoss: 0.083082\n",
      "Train Epoch: 12 [14400/48000 (30%)]\tLoss: 0.095619\n",
      "Train Epoch: 12 [19200/48000 (40%)]\tLoss: 0.081570\n",
      "Train Epoch: 12 [24000/48000 (50%)]\tLoss: 0.075467\n",
      "Train Epoch: 12 [28800/48000 (60%)]\tLoss: 0.191252\n",
      "Train Epoch: 12 [33600/48000 (70%)]\tLoss: 0.019355\n",
      "Train Epoch: 12 [38400/48000 (80%)]\tLoss: 0.111861\n",
      "Train Epoch: 12 [43200/48000 (90%)]\tLoss: 0.091209\n",
      "Epoch 12, Val Loss: 0.001032, Accuracy: 11883/12000 (99.03%)\n",
      "Train Epoch: 13 [0/48000 (0%)]\tLoss: 0.056138\n",
      "Train Epoch: 13 [4800/48000 (10%)]\tLoss: 0.018086\n",
      "Train Epoch: 13 [9600/48000 (20%)]\tLoss: 0.009029\n",
      "Train Epoch: 13 [14400/48000 (30%)]\tLoss: 0.273821\n",
      "Train Epoch: 13 [19200/48000 (40%)]\tLoss: 0.017529\n",
      "Train Epoch: 13 [24000/48000 (50%)]\tLoss: 0.076990\n",
      "Train Epoch: 13 [28800/48000 (60%)]\tLoss: 0.011892\n",
      "Train Epoch: 13 [33600/48000 (70%)]\tLoss: 0.037660\n",
      "Train Epoch: 13 [38400/48000 (80%)]\tLoss: 0.056854\n",
      "Train Epoch: 13 [43200/48000 (90%)]\tLoss: 0.019707\n",
      "Epoch 13, Val Loss: 0.001022, Accuracy: 11880/12000 (99.00%)\n",
      "Train Epoch: 14 [0/48000 (0%)]\tLoss: 0.041223\n",
      "Train Epoch: 14 [4800/48000 (10%)]\tLoss: 0.111833\n",
      "Train Epoch: 14 [9600/48000 (20%)]\tLoss: 0.019443\n",
      "Train Epoch: 14 [14400/48000 (30%)]\tLoss: 0.036960\n",
      "Train Epoch: 14 [19200/48000 (40%)]\tLoss: 0.144289\n",
      "Train Epoch: 14 [24000/48000 (50%)]\tLoss: 0.026090\n",
      "Train Epoch: 14 [28800/48000 (60%)]\tLoss: 0.076024\n",
      "Train Epoch: 14 [33600/48000 (70%)]\tLoss: 0.121311\n",
      "Train Epoch: 14 [38400/48000 (80%)]\tLoss: 0.107505\n",
      "Train Epoch: 14 [43200/48000 (90%)]\tLoss: 0.107556\n",
      "Epoch 14, Val Loss: 0.000986, Accuracy: 11887/12000 (99.06%)\n",
      "Train Epoch: 15 [0/48000 (0%)]\tLoss: 0.059987\n",
      "Train Epoch: 15 [4800/48000 (10%)]\tLoss: 0.034714\n",
      "Train Epoch: 15 [9600/48000 (20%)]\tLoss: 0.052244\n",
      "Train Epoch: 15 [14400/48000 (30%)]\tLoss: 0.078893\n",
      "Train Epoch: 15 [19200/48000 (40%)]\tLoss: 0.188189\n",
      "Train Epoch: 15 [24000/48000 (50%)]\tLoss: 0.028101\n",
      "Train Epoch: 15 [28800/48000 (60%)]\tLoss: 0.071534\n",
      "Train Epoch: 15 [33600/48000 (70%)]\tLoss: 0.044170\n",
      "Train Epoch: 15 [38400/48000 (80%)]\tLoss: 0.115495\n",
      "Train Epoch: 15 [43200/48000 (90%)]\tLoss: 0.087382\n",
      "Epoch 15, Val Loss: 0.000970, Accuracy: 11879/12000 (98.99%)\n",
      "Train Epoch: 16 [0/48000 (0%)]\tLoss: 0.034516\n",
      "Train Epoch: 16 [4800/48000 (10%)]\tLoss: 0.036956\n",
      "Train Epoch: 16 [9600/48000 (20%)]\tLoss: 0.063584\n",
      "Train Epoch: 16 [14400/48000 (30%)]\tLoss: 0.047229\n",
      "Train Epoch: 16 [19200/48000 (40%)]\tLoss: 0.037655\n",
      "Train Epoch: 16 [24000/48000 (50%)]\tLoss: 0.019349\n",
      "Train Epoch: 16 [28800/48000 (60%)]\tLoss: 0.029447\n",
      "Train Epoch: 16 [33600/48000 (70%)]\tLoss: 0.111907\n",
      "Train Epoch: 16 [38400/48000 (80%)]\tLoss: 0.125253\n",
      "Train Epoch: 16 [43200/48000 (90%)]\tLoss: 0.039522\n",
      "Epoch 16, Val Loss: 0.000966, Accuracy: 11885/12000 (99.04%)\n",
      "Train Epoch: 17 [0/48000 (0%)]\tLoss: 0.091014\n",
      "Train Epoch: 17 [4800/48000 (10%)]\tLoss: 0.034338\n",
      "Train Epoch: 17 [9600/48000 (20%)]\tLoss: 0.051483\n",
      "Train Epoch: 17 [14400/48000 (30%)]\tLoss: 0.097802\n",
      "Train Epoch: 17 [19200/48000 (40%)]\tLoss: 0.016886\n",
      "Train Epoch: 17 [24000/48000 (50%)]\tLoss: 0.028475\n",
      "Train Epoch: 17 [28800/48000 (60%)]\tLoss: 0.015105\n",
      "Train Epoch: 17 [33600/48000 (70%)]\tLoss: 0.020989\n",
      "Train Epoch: 17 [38400/48000 (80%)]\tLoss: 0.149423\n",
      "Train Epoch: 17 [43200/48000 (90%)]\tLoss: 0.061833\n",
      "Epoch 17, Val Loss: 0.000946, Accuracy: 11885/12000 (99.04%)\n",
      "Train Epoch: 18 [0/48000 (0%)]\tLoss: 0.025291\n",
      "Train Epoch: 18 [4800/48000 (10%)]\tLoss: 0.083282\n",
      "Train Epoch: 18 [9600/48000 (20%)]\tLoss: 0.316481\n",
      "Train Epoch: 18 [14400/48000 (30%)]\tLoss: 0.058750\n",
      "Train Epoch: 18 [19200/48000 (40%)]\tLoss: 0.068825\n",
      "Train Epoch: 18 [24000/48000 (50%)]\tLoss: 0.070822\n",
      "Train Epoch: 18 [28800/48000 (60%)]\tLoss: 0.042525\n",
      "Train Epoch: 18 [33600/48000 (70%)]\tLoss: 0.223014\n",
      "Train Epoch: 18 [38400/48000 (80%)]\tLoss: 0.053487\n",
      "Train Epoch: 18 [43200/48000 (90%)]\tLoss: 0.089085\n",
      "Epoch 18, Val Loss: 0.000926, Accuracy: 11886/12000 (99.05%)\n",
      "Train Epoch: 19 [0/48000 (0%)]\tLoss: 0.055492\n",
      "Train Epoch: 19 [4800/48000 (10%)]\tLoss: 0.091312\n",
      "Train Epoch: 19 [9600/48000 (20%)]\tLoss: 0.022100\n",
      "Train Epoch: 19 [14400/48000 (30%)]\tLoss: 0.052412\n",
      "Train Epoch: 19 [19200/48000 (40%)]\tLoss: 0.011152\n",
      "Train Epoch: 19 [24000/48000 (50%)]\tLoss: 0.009124\n",
      "Train Epoch: 19 [28800/48000 (60%)]\tLoss: 0.012343\n",
      "Train Epoch: 19 [33600/48000 (70%)]\tLoss: 0.106467\n",
      "Train Epoch: 19 [38400/48000 (80%)]\tLoss: 0.016397\n",
      "Train Epoch: 19 [43200/48000 (90%)]\tLoss: 0.042273\n",
      "Epoch 19, Val Loss: 0.000925, Accuracy: 11891/12000 (99.09%)\n",
      "Train Epoch: 20 [0/48000 (0%)]\tLoss: 0.067364\n",
      "Train Epoch: 20 [4800/48000 (10%)]\tLoss: 0.021335\n",
      "Train Epoch: 20 [9600/48000 (20%)]\tLoss: 0.046050\n",
      "Train Epoch: 20 [14400/48000 (30%)]\tLoss: 0.016645\n",
      "Train Epoch: 20 [19200/48000 (40%)]\tLoss: 0.010629\n",
      "Train Epoch: 20 [24000/48000 (50%)]\tLoss: 0.016664\n",
      "Train Epoch: 20 [28800/48000 (60%)]\tLoss: 0.126385\n",
      "Train Epoch: 20 [33600/48000 (70%)]\tLoss: 0.041277\n",
      "Train Epoch: 20 [38400/48000 (80%)]\tLoss: 0.017728\n",
      "Train Epoch: 20 [43200/48000 (90%)]\tLoss: 0.155086\n",
      "Epoch 20, Val Loss: 0.000909, Accuracy: 11888/12000 (99.07%)\n",
      "Train Epoch: 21 [0/48000 (0%)]\tLoss: 0.019806\n",
      "Train Epoch: 21 [4800/48000 (10%)]\tLoss: 0.068294\n",
      "Train Epoch: 21 [9600/48000 (20%)]\tLoss: 0.040889\n",
      "Train Epoch: 21 [14400/48000 (30%)]\tLoss: 0.093947\n",
      "Train Epoch: 21 [19200/48000 (40%)]\tLoss: 0.013381\n",
      "Train Epoch: 21 [24000/48000 (50%)]\tLoss: 0.116350\n",
      "Train Epoch: 21 [28800/48000 (60%)]\tLoss: 0.027915\n",
      "Train Epoch: 21 [33600/48000 (70%)]\tLoss: 0.075581\n",
      "Train Epoch: 21 [38400/48000 (80%)]\tLoss: 0.004732\n",
      "Train Epoch: 21 [43200/48000 (90%)]\tLoss: 0.007626\n",
      "Epoch 21, Val Loss: 0.000904, Accuracy: 11889/12000 (99.08%)\n",
      "Train Epoch: 22 [0/48000 (0%)]\tLoss: 0.173395\n",
      "Train Epoch: 22 [4800/48000 (10%)]\tLoss: 0.071460\n",
      "Train Epoch: 22 [9600/48000 (20%)]\tLoss: 0.019867\n",
      "Train Epoch: 22 [14400/48000 (30%)]\tLoss: 0.069748\n",
      "Train Epoch: 22 [19200/48000 (40%)]\tLoss: 0.037642\n",
      "Train Epoch: 22 [24000/48000 (50%)]\tLoss: 0.103108\n",
      "Train Epoch: 22 [28800/48000 (60%)]\tLoss: 0.036207\n",
      "Train Epoch: 22 [33600/48000 (70%)]\tLoss: 0.034745\n",
      "Train Epoch: 22 [38400/48000 (80%)]\tLoss: 0.062458\n",
      "Train Epoch: 22 [43200/48000 (90%)]\tLoss: 0.033016\n",
      "Epoch 22, Val Loss: 0.000906, Accuracy: 11894/12000 (99.12%)\n",
      "Train Epoch: 23 [0/48000 (0%)]\tLoss: 0.011903\n",
      "Train Epoch: 23 [4800/48000 (10%)]\tLoss: 0.017445\n",
      "Train Epoch: 23 [9600/48000 (20%)]\tLoss: 0.007557\n",
      "Train Epoch: 23 [14400/48000 (30%)]\tLoss: 0.027653\n",
      "Train Epoch: 23 [19200/48000 (40%)]\tLoss: 0.119045\n",
      "Train Epoch: 23 [24000/48000 (50%)]\tLoss: 0.014516\n",
      "Train Epoch: 23 [28800/48000 (60%)]\tLoss: 0.022930\n",
      "Train Epoch: 23 [33600/48000 (70%)]\tLoss: 0.070086\n",
      "Train Epoch: 23 [38400/48000 (80%)]\tLoss: 0.026402\n",
      "Train Epoch: 23 [43200/48000 (90%)]\tLoss: 0.228236\n",
      "Epoch 23, Val Loss: 0.000896, Accuracy: 11896/12000 (99.13%)\n",
      "Train Epoch: 24 [0/48000 (0%)]\tLoss: 0.088579\n",
      "Train Epoch: 24 [4800/48000 (10%)]\tLoss: 0.060867\n",
      "Train Epoch: 24 [9600/48000 (20%)]\tLoss: 0.037057\n",
      "Train Epoch: 24 [14400/48000 (30%)]\tLoss: 0.049405\n",
      "Train Epoch: 24 [19200/48000 (40%)]\tLoss: 0.032727\n",
      "Train Epoch: 24 [24000/48000 (50%)]\tLoss: 0.013865\n",
      "Train Epoch: 24 [28800/48000 (60%)]\tLoss: 0.029832\n",
      "Train Epoch: 24 [33600/48000 (70%)]\tLoss: 0.436670\n",
      "Train Epoch: 24 [38400/48000 (80%)]\tLoss: 0.018480\n",
      "Train Epoch: 24 [43200/48000 (90%)]\tLoss: 0.032642\n",
      "Epoch 24, Val Loss: 0.000891, Accuracy: 11898/12000 (99.15%)\n",
      "Train Epoch: 25 [0/48000 (0%)]\tLoss: 0.033172\n",
      "Train Epoch: 25 [4800/48000 (10%)]\tLoss: 0.096074\n",
      "Train Epoch: 25 [9600/48000 (20%)]\tLoss: 0.063531\n",
      "Train Epoch: 25 [14400/48000 (30%)]\tLoss: 0.041458\n",
      "Train Epoch: 25 [19200/48000 (40%)]\tLoss: 0.032666\n",
      "Train Epoch: 25 [24000/48000 (50%)]\tLoss: 0.021548\n",
      "Train Epoch: 25 [28800/48000 (60%)]\tLoss: 0.062713\n",
      "Train Epoch: 25 [33600/48000 (70%)]\tLoss: 0.031604\n",
      "Train Epoch: 25 [38400/48000 (80%)]\tLoss: 0.164444\n",
      "Train Epoch: 25 [43200/48000 (90%)]\tLoss: 0.016364\n",
      "Epoch 25, Val Loss: 0.000883, Accuracy: 11894/12000 (99.12%)\n",
      "Train Epoch: 26 [0/48000 (0%)]\tLoss: 0.030791\n",
      "Train Epoch: 26 [4800/48000 (10%)]\tLoss: 0.006159\n",
      "Train Epoch: 26 [9600/48000 (20%)]\tLoss: 0.055227\n",
      "Train Epoch: 26 [14400/48000 (30%)]\tLoss: 0.023548\n",
      "Train Epoch: 26 [19200/48000 (40%)]\tLoss: 0.010420\n",
      "Train Epoch: 26 [24000/48000 (50%)]\tLoss: 0.042663\n",
      "Train Epoch: 26 [28800/48000 (60%)]\tLoss: 0.042482\n",
      "Train Epoch: 26 [33600/48000 (70%)]\tLoss: 0.039018\n",
      "Train Epoch: 26 [38400/48000 (80%)]\tLoss: 0.015907\n",
      "Train Epoch: 26 [43200/48000 (90%)]\tLoss: 0.050441\n",
      "Epoch 26, Val Loss: 0.000883, Accuracy: 11898/12000 (99.15%)\n",
      "Train Epoch: 27 [0/48000 (0%)]\tLoss: 0.032405\n",
      "Train Epoch: 27 [4800/48000 (10%)]\tLoss: 0.055352\n",
      "Train Epoch: 27 [9600/48000 (20%)]\tLoss: 0.045906\n",
      "Train Epoch: 27 [14400/48000 (30%)]\tLoss: 0.007427\n",
      "Train Epoch: 27 [19200/48000 (40%)]\tLoss: 0.017421\n",
      "Train Epoch: 27 [24000/48000 (50%)]\tLoss: 0.069278\n",
      "Train Epoch: 27 [28800/48000 (60%)]\tLoss: 0.027411\n",
      "Train Epoch: 27 [33600/48000 (70%)]\tLoss: 0.066799\n",
      "Train Epoch: 27 [38400/48000 (80%)]\tLoss: 0.090727\n",
      "Train Epoch: 27 [43200/48000 (90%)]\tLoss: 0.219379\n",
      "Epoch 27, Val Loss: 0.000870, Accuracy: 11897/12000 (99.14%)\n",
      "Train Epoch: 28 [0/48000 (0%)]\tLoss: 0.013219\n",
      "Train Epoch: 28 [4800/48000 (10%)]\tLoss: 0.078370\n",
      "Train Epoch: 28 [9600/48000 (20%)]\tLoss: 0.039819\n",
      "Train Epoch: 28 [14400/48000 (30%)]\tLoss: 0.015882\n",
      "Train Epoch: 28 [19200/48000 (40%)]\tLoss: 0.006020\n",
      "Train Epoch: 28 [24000/48000 (50%)]\tLoss: 0.142435\n",
      "Train Epoch: 28 [28800/48000 (60%)]\tLoss: 0.154689\n",
      "Train Epoch: 28 [33600/48000 (70%)]\tLoss: 0.007984\n",
      "Train Epoch: 28 [38400/48000 (80%)]\tLoss: 0.090604\n",
      "Train Epoch: 28 [43200/48000 (90%)]\tLoss: 0.064537\n",
      "Epoch 28, Val Loss: 0.000859, Accuracy: 11900/12000 (99.17%)\n",
      "Train Epoch: 29 [0/48000 (0%)]\tLoss: 0.061764\n",
      "Train Epoch: 29 [4800/48000 (10%)]\tLoss: 0.011090\n",
      "Train Epoch: 29 [9600/48000 (20%)]\tLoss: 0.025189\n",
      "Train Epoch: 29 [14400/48000 (30%)]\tLoss: 0.005969\n",
      "Train Epoch: 29 [19200/48000 (40%)]\tLoss: 0.012658\n",
      "Train Epoch: 29 [24000/48000 (50%)]\tLoss: 0.076272\n",
      "Train Epoch: 29 [28800/48000 (60%)]\tLoss: 0.180817\n",
      "Train Epoch: 29 [33600/48000 (70%)]\tLoss: 0.060667\n",
      "Train Epoch: 29 [38400/48000 (80%)]\tLoss: 0.052600\n",
      "Train Epoch: 29 [43200/48000 (90%)]\tLoss: 0.018567\n",
      "Epoch 29, Val Loss: 0.000868, Accuracy: 11895/12000 (99.12%)\n",
      "Train Epoch: 30 [0/48000 (0%)]\tLoss: 0.048102\n",
      "Train Epoch: 30 [4800/48000 (10%)]\tLoss: 0.033756\n",
      "Train Epoch: 30 [9600/48000 (20%)]\tLoss: 0.002437\n",
      "Train Epoch: 30 [14400/48000 (30%)]\tLoss: 0.027222\n",
      "Train Epoch: 30 [19200/48000 (40%)]\tLoss: 0.077768\n",
      "Train Epoch: 30 [24000/48000 (50%)]\tLoss: 0.035603\n",
      "Train Epoch: 30 [28800/48000 (60%)]\tLoss: 0.058174\n",
      "Train Epoch: 30 [33600/48000 (70%)]\tLoss: 0.012060\n",
      "Train Epoch: 30 [38400/48000 (80%)]\tLoss: 0.013078\n",
      "Train Epoch: 30 [43200/48000 (90%)]\tLoss: 0.019356\n",
      "Epoch 30, Val Loss: 0.000859, Accuracy: 11897/12000 (99.14%)\n",
      "Train Epoch: 31 [0/48000 (0%)]\tLoss: 0.032843\n",
      "Train Epoch: 31 [4800/48000 (10%)]\tLoss: 0.013552\n",
      "Train Epoch: 31 [9600/48000 (20%)]\tLoss: 0.013609\n",
      "Train Epoch: 31 [14400/48000 (30%)]\tLoss: 0.045476\n",
      "Train Epoch: 31 [19200/48000 (40%)]\tLoss: 0.024002\n",
      "Train Epoch: 31 [24000/48000 (50%)]\tLoss: 0.134923\n",
      "Train Epoch: 31 [28800/48000 (60%)]\tLoss: 0.043926\n",
      "Train Epoch: 31 [33600/48000 (70%)]\tLoss: 0.027713\n",
      "Train Epoch: 31 [38400/48000 (80%)]\tLoss: 0.016430\n",
      "Train Epoch: 31 [43200/48000 (90%)]\tLoss: 0.128770\n",
      "Epoch 31, Val Loss: 0.000848, Accuracy: 11894/12000 (99.12%)\n",
      "Train Epoch: 32 [0/48000 (0%)]\tLoss: 0.058914\n",
      "Train Epoch: 32 [4800/48000 (10%)]\tLoss: 0.006271\n",
      "Train Epoch: 32 [9600/48000 (20%)]\tLoss: 0.065206\n",
      "Train Epoch: 32 [14400/48000 (30%)]\tLoss: 0.036496\n",
      "Train Epoch: 32 [19200/48000 (40%)]\tLoss: 0.118961\n",
      "Train Epoch: 32 [24000/48000 (50%)]\tLoss: 0.005605\n",
      "Train Epoch: 32 [28800/48000 (60%)]\tLoss: 0.066818\n",
      "Train Epoch: 32 [33600/48000 (70%)]\tLoss: 0.036344\n",
      "Train Epoch: 32 [38400/48000 (80%)]\tLoss: 0.028196\n",
      "Train Epoch: 32 [43200/48000 (90%)]\tLoss: 0.059014\n",
      "Epoch 32, Val Loss: 0.000861, Accuracy: 11896/12000 (99.13%)\n",
      "Train Epoch: 33 [0/48000 (0%)]\tLoss: 0.009804\n",
      "Train Epoch: 33 [4800/48000 (10%)]\tLoss: 0.005916\n",
      "Train Epoch: 33 [9600/48000 (20%)]\tLoss: 0.007431\n",
      "Train Epoch: 33 [14400/48000 (30%)]\tLoss: 0.015722\n",
      "Train Epoch: 33 [19200/48000 (40%)]\tLoss: 0.048829\n",
      "Train Epoch: 33 [24000/48000 (50%)]\tLoss: 0.005278\n",
      "Train Epoch: 33 [28800/48000 (60%)]\tLoss: 0.014892\n",
      "Train Epoch: 33 [33600/48000 (70%)]\tLoss: 0.049862\n",
      "Train Epoch: 33 [38400/48000 (80%)]\tLoss: 0.034107\n",
      "Train Epoch: 33 [43200/48000 (90%)]\tLoss: 0.026336\n",
      "Epoch 33, Val Loss: 0.000837, Accuracy: 11896/12000 (99.13%)\n",
      "Train Epoch: 34 [0/48000 (0%)]\tLoss: 0.108702\n",
      "Train Epoch: 34 [4800/48000 (10%)]\tLoss: 0.002347\n",
      "Train Epoch: 34 [9600/48000 (20%)]\tLoss: 0.093344\n",
      "Train Epoch: 34 [14400/48000 (30%)]\tLoss: 0.064661\n",
      "Train Epoch: 34 [19200/48000 (40%)]\tLoss: 0.012951\n",
      "Train Epoch: 34 [24000/48000 (50%)]\tLoss: 0.027178\n",
      "Train Epoch: 34 [28800/48000 (60%)]\tLoss: 0.032937\n",
      "Train Epoch: 34 [33600/48000 (70%)]\tLoss: 0.127646\n",
      "Train Epoch: 34 [38400/48000 (80%)]\tLoss: 0.034362\n",
      "Train Epoch: 34 [43200/48000 (90%)]\tLoss: 0.052404\n",
      "Epoch 34, Val Loss: 0.000828, Accuracy: 11902/12000 (99.18%)\n",
      "Train Epoch: 35 [0/48000 (0%)]\tLoss: 0.004668\n",
      "Train Epoch: 35 [4800/48000 (10%)]\tLoss: 0.022862\n",
      "Train Epoch: 35 [9600/48000 (20%)]\tLoss: 0.005221\n",
      "Train Epoch: 35 [14400/48000 (30%)]\tLoss: 0.019262\n",
      "Train Epoch: 35 [19200/48000 (40%)]\tLoss: 0.022490\n",
      "Train Epoch: 35 [24000/48000 (50%)]\tLoss: 0.041818\n",
      "Train Epoch: 35 [28800/48000 (60%)]\tLoss: 0.024810\n",
      "Train Epoch: 35 [33600/48000 (70%)]\tLoss: 0.002943\n",
      "Train Epoch: 35 [38400/48000 (80%)]\tLoss: 0.019579\n",
      "Train Epoch: 35 [43200/48000 (90%)]\tLoss: 0.010950\n",
      "Epoch 35, Val Loss: 0.000872, Accuracy: 11905/12000 (99.21%)\n",
      "Train Epoch: 36 [0/48000 (0%)]\tLoss: 0.001479\n",
      "Train Epoch: 36 [4800/48000 (10%)]\tLoss: 0.014937\n",
      "Train Epoch: 36 [9600/48000 (20%)]\tLoss: 0.046769\n",
      "Train Epoch: 36 [14400/48000 (30%)]\tLoss: 0.016771\n",
      "Train Epoch: 36 [19200/48000 (40%)]\tLoss: 0.068226\n",
      "Train Epoch: 36 [24000/48000 (50%)]\tLoss: 0.049398\n",
      "Train Epoch: 36 [28800/48000 (60%)]\tLoss: 0.028147\n",
      "Train Epoch: 36 [33600/48000 (70%)]\tLoss: 0.065177\n",
      "Train Epoch: 36 [38400/48000 (80%)]\tLoss: 0.070989\n",
      "Train Epoch: 36 [43200/48000 (90%)]\tLoss: 0.021653\n",
      "Epoch 36, Val Loss: 0.000841, Accuracy: 11902/12000 (99.18%)\n",
      "Train Epoch: 37 [0/48000 (0%)]\tLoss: 0.004018\n",
      "Train Epoch: 37 [4800/48000 (10%)]\tLoss: 0.016092\n",
      "Train Epoch: 37 [9600/48000 (20%)]\tLoss: 0.047590\n",
      "Train Epoch: 37 [14400/48000 (30%)]\tLoss: 0.095760\n",
      "Train Epoch: 37 [19200/48000 (40%)]\tLoss: 0.009953\n",
      "Train Epoch: 37 [24000/48000 (50%)]\tLoss: 0.060668\n",
      "Train Epoch: 37 [28800/48000 (60%)]\tLoss: 0.010996\n",
      "Train Epoch: 37 [33600/48000 (70%)]\tLoss: 0.012482\n",
      "Train Epoch: 37 [38400/48000 (80%)]\tLoss: 0.059638\n",
      "Train Epoch: 37 [43200/48000 (90%)]\tLoss: 0.102413\n",
      "Epoch 37, Val Loss: 0.000821, Accuracy: 11904/12000 (99.20%)\n",
      "Train Epoch: 38 [0/48000 (0%)]\tLoss: 0.065940\n",
      "Train Epoch: 38 [4800/48000 (10%)]\tLoss: 0.017270\n",
      "Train Epoch: 38 [9600/48000 (20%)]\tLoss: 0.068010\n",
      "Train Epoch: 38 [14400/48000 (30%)]\tLoss: 0.105325\n",
      "Train Epoch: 38 [19200/48000 (40%)]\tLoss: 0.042970\n",
      "Train Epoch: 38 [24000/48000 (50%)]\tLoss: 0.027208\n",
      "Train Epoch: 38 [28800/48000 (60%)]\tLoss: 0.007639\n",
      "Train Epoch: 38 [33600/48000 (70%)]\tLoss: 0.071485\n",
      "Train Epoch: 38 [38400/48000 (80%)]\tLoss: 0.112172\n",
      "Train Epoch: 38 [43200/48000 (90%)]\tLoss: 0.004829\n",
      "Epoch 38, Val Loss: 0.000815, Accuracy: 11908/12000 (99.23%)\n",
      "Train Epoch: 39 [0/48000 (0%)]\tLoss: 0.101896\n",
      "Train Epoch: 39 [4800/48000 (10%)]\tLoss: 0.029466\n",
      "Train Epoch: 39 [9600/48000 (20%)]\tLoss: 0.135931\n",
      "Train Epoch: 39 [14400/48000 (30%)]\tLoss: 0.026627\n",
      "Train Epoch: 39 [19200/48000 (40%)]\tLoss: 0.032876\n",
      "Train Epoch: 39 [24000/48000 (50%)]\tLoss: 0.097165\n",
      "Train Epoch: 39 [28800/48000 (60%)]\tLoss: 0.003316\n",
      "Train Epoch: 39 [33600/48000 (70%)]\tLoss: 0.142668\n",
      "Train Epoch: 39 [38400/48000 (80%)]\tLoss: 0.084525\n",
      "Train Epoch: 39 [43200/48000 (90%)]\tLoss: 0.011846\n",
      "Epoch 39, Val Loss: 0.000820, Accuracy: 11898/12000 (99.15%)\n",
      "Train Epoch: 40 [0/48000 (0%)]\tLoss: 0.006179\n",
      "Train Epoch: 40 [4800/48000 (10%)]\tLoss: 0.085583\n",
      "Train Epoch: 40 [9600/48000 (20%)]\tLoss: 0.018041\n",
      "Train Epoch: 40 [14400/48000 (30%)]\tLoss: 0.011091\n",
      "Train Epoch: 40 [19200/48000 (40%)]\tLoss: 0.018975\n",
      "Train Epoch: 40 [24000/48000 (50%)]\tLoss: 0.019689\n",
      "Train Epoch: 40 [28800/48000 (60%)]\tLoss: 0.029924\n",
      "Train Epoch: 40 [33600/48000 (70%)]\tLoss: 0.050855\n",
      "Train Epoch: 40 [38400/48000 (80%)]\tLoss: 0.127601\n",
      "Train Epoch: 40 [43200/48000 (90%)]\tLoss: 0.004060\n",
      "Epoch 40, Val Loss: 0.000813, Accuracy: 11906/12000 (99.22%)\n",
      "Train Epoch: 41 [0/48000 (0%)]\tLoss: 0.021464\n",
      "Train Epoch: 41 [4800/48000 (10%)]\tLoss: 0.017723\n",
      "Train Epoch: 41 [9600/48000 (20%)]\tLoss: 0.008263\n",
      "Train Epoch: 41 [14400/48000 (30%)]\tLoss: 0.016001\n",
      "Train Epoch: 41 [19200/48000 (40%)]\tLoss: 0.012427\n",
      "Train Epoch: 41 [24000/48000 (50%)]\tLoss: 0.006112\n",
      "Train Epoch: 41 [28800/48000 (60%)]\tLoss: 0.003574\n",
      "Train Epoch: 41 [33600/48000 (70%)]\tLoss: 0.006065\n",
      "Train Epoch: 41 [38400/48000 (80%)]\tLoss: 0.016180\n",
      "Train Epoch: 41 [43200/48000 (90%)]\tLoss: 0.089185\n",
      "Epoch 41, Val Loss: 0.000813, Accuracy: 11908/12000 (99.23%)\n",
      "Train Epoch: 42 [0/48000 (0%)]\tLoss: 0.036914\n",
      "Train Epoch: 42 [4800/48000 (10%)]\tLoss: 0.011454\n",
      "Train Epoch: 42 [9600/48000 (20%)]\tLoss: 0.041399\n",
      "Train Epoch: 42 [14400/48000 (30%)]\tLoss: 0.010592\n",
      "Train Epoch: 42 [19200/48000 (40%)]\tLoss: 0.045165\n",
      "Train Epoch: 42 [24000/48000 (50%)]\tLoss: 0.004669\n",
      "Train Epoch: 42 [28800/48000 (60%)]\tLoss: 0.023597\n",
      "Train Epoch: 42 [33600/48000 (70%)]\tLoss: 0.003414\n",
      "Train Epoch: 42 [38400/48000 (80%)]\tLoss: 0.076465\n",
      "Train Epoch: 42 [43200/48000 (90%)]\tLoss: 0.014853\n",
      "Epoch 42, Val Loss: 0.000814, Accuracy: 11909/12000 (99.24%)\n",
      "Train Epoch: 43 [0/48000 (0%)]\tLoss: 0.025135\n",
      "Train Epoch: 43 [4800/48000 (10%)]\tLoss: 0.003623\n",
      "Train Epoch: 43 [9600/48000 (20%)]\tLoss: 0.013638\n",
      "Train Epoch: 43 [14400/48000 (30%)]\tLoss: 0.117990\n",
      "Train Epoch: 43 [19200/48000 (40%)]\tLoss: 0.012900\n",
      "Train Epoch: 43 [24000/48000 (50%)]\tLoss: 0.004648\n",
      "Train Epoch: 43 [28800/48000 (60%)]\tLoss: 0.026053\n",
      "Train Epoch: 43 [33600/48000 (70%)]\tLoss: 0.010273\n",
      "Train Epoch: 43 [38400/48000 (80%)]\tLoss: 0.035424\n",
      "Train Epoch: 43 [43200/48000 (90%)]\tLoss: 0.004039\n",
      "Epoch 43, Val Loss: 0.000808, Accuracy: 11912/12000 (99.27%)\n",
      "Train Epoch: 44 [0/48000 (0%)]\tLoss: 0.071504\n",
      "Train Epoch: 44 [4800/48000 (10%)]\tLoss: 0.034934\n",
      "Train Epoch: 44 [9600/48000 (20%)]\tLoss: 0.025966\n",
      "Train Epoch: 44 [14400/48000 (30%)]\tLoss: 0.006957\n",
      "Train Epoch: 44 [19200/48000 (40%)]\tLoss: 0.002522\n",
      "Train Epoch: 44 [24000/48000 (50%)]\tLoss: 0.082278\n",
      "Train Epoch: 44 [28800/48000 (60%)]\tLoss: 0.019164\n",
      "Train Epoch: 44 [33600/48000 (70%)]\tLoss: 0.012539\n",
      "Train Epoch: 44 [38400/48000 (80%)]\tLoss: 0.061765\n",
      "Train Epoch: 44 [43200/48000 (90%)]\tLoss: 0.014033\n",
      "Epoch 44, Val Loss: 0.000813, Accuracy: 11905/12000 (99.21%)\n",
      "Train Epoch: 45 [0/48000 (0%)]\tLoss: 0.099691\n",
      "Train Epoch: 45 [4800/48000 (10%)]\tLoss: 0.002251\n",
      "Train Epoch: 45 [9600/48000 (20%)]\tLoss: 0.004074\n",
      "Train Epoch: 45 [14400/48000 (30%)]\tLoss: 0.044467\n",
      "Train Epoch: 45 [19200/48000 (40%)]\tLoss: 0.109963\n",
      "Train Epoch: 45 [24000/48000 (50%)]\tLoss: 0.005397\n",
      "Train Epoch: 45 [28800/48000 (60%)]\tLoss: 0.037091\n",
      "Train Epoch: 45 [33600/48000 (70%)]\tLoss: 0.000965\n",
      "Train Epoch: 45 [38400/48000 (80%)]\tLoss: 0.053562\n",
      "Train Epoch: 45 [43200/48000 (90%)]\tLoss: 0.020212\n",
      "Epoch 45, Val Loss: 0.000807, Accuracy: 11911/12000 (99.26%)\n",
      "Train Epoch: 46 [0/48000 (0%)]\tLoss: 0.036091\n",
      "Train Epoch: 46 [4800/48000 (10%)]\tLoss: 0.005570\n",
      "Train Epoch: 46 [9600/48000 (20%)]\tLoss: 0.004693\n",
      "Train Epoch: 46 [14400/48000 (30%)]\tLoss: 0.019040\n",
      "Train Epoch: 46 [19200/48000 (40%)]\tLoss: 0.099637\n",
      "Train Epoch: 46 [24000/48000 (50%)]\tLoss: 0.003399\n",
      "Train Epoch: 46 [28800/48000 (60%)]\tLoss: 0.017901\n",
      "Train Epoch: 46 [33600/48000 (70%)]\tLoss: 0.024009\n",
      "Train Epoch: 46 [38400/48000 (80%)]\tLoss: 0.007207\n",
      "Train Epoch: 46 [43200/48000 (90%)]\tLoss: 0.205310\n",
      "Epoch 46, Val Loss: 0.000811, Accuracy: 11912/12000 (99.27%)\n",
      "Train Epoch: 47 [0/48000 (0%)]\tLoss: 0.003150\n",
      "Train Epoch: 47 [4800/48000 (10%)]\tLoss: 0.132972\n",
      "Train Epoch: 47 [9600/48000 (20%)]\tLoss: 0.080506\n",
      "Train Epoch: 47 [14400/48000 (30%)]\tLoss: 0.033350\n",
      "Train Epoch: 47 [19200/48000 (40%)]\tLoss: 0.044268\n",
      "Train Epoch: 47 [24000/48000 (50%)]\tLoss: 0.009559\n",
      "Train Epoch: 47 [28800/48000 (60%)]\tLoss: 0.051303\n",
      "Train Epoch: 47 [33600/48000 (70%)]\tLoss: 0.003991\n",
      "Train Epoch: 47 [38400/48000 (80%)]\tLoss: 0.003254\n",
      "Train Epoch: 47 [43200/48000 (90%)]\tLoss: 0.028067\n",
      "Epoch 47, Val Loss: 0.000789, Accuracy: 11911/12000 (99.26%)\n",
      "Train Epoch: 48 [0/48000 (0%)]\tLoss: 0.045145\n",
      "Train Epoch: 48 [4800/48000 (10%)]\tLoss: 0.064260\n",
      "Train Epoch: 48 [9600/48000 (20%)]\tLoss: 0.006027\n",
      "Train Epoch: 48 [14400/48000 (30%)]\tLoss: 0.012844\n",
      "Train Epoch: 48 [19200/48000 (40%)]\tLoss: 0.001214\n",
      "Train Epoch: 48 [24000/48000 (50%)]\tLoss: 0.033231\n",
      "Train Epoch: 48 [28800/48000 (60%)]\tLoss: 0.147999\n",
      "Train Epoch: 48 [33600/48000 (70%)]\tLoss: 0.025397\n",
      "Train Epoch: 48 [38400/48000 (80%)]\tLoss: 0.033295\n",
      "Train Epoch: 48 [43200/48000 (90%)]\tLoss: 0.067888\n",
      "Epoch 48, Val Loss: 0.000819, Accuracy: 11902/12000 (99.18%)\n",
      "Train Epoch: 49 [0/48000 (0%)]\tLoss: 0.042877\n",
      "Train Epoch: 49 [4800/48000 (10%)]\tLoss: 0.030141\n",
      "Train Epoch: 49 [9600/48000 (20%)]\tLoss: 0.013379\n",
      "Train Epoch: 49 [14400/48000 (30%)]\tLoss: 0.004406\n",
      "Train Epoch: 49 [19200/48000 (40%)]\tLoss: 0.029386\n",
      "Train Epoch: 49 [24000/48000 (50%)]\tLoss: 0.006873\n",
      "Train Epoch: 49 [28800/48000 (60%)]\tLoss: 0.015221\n",
      "Train Epoch: 49 [33600/48000 (70%)]\tLoss: 0.090958\n",
      "Train Epoch: 49 [38400/48000 (80%)]\tLoss: 0.042862\n",
      "Train Epoch: 49 [43200/48000 (90%)]\tLoss: 0.019636\n",
      "Epoch 49, Val Loss: 0.000788, Accuracy: 11910/12000 (99.25%)\n",
      "Train Epoch: 50 [0/48000 (0%)]\tLoss: 0.016438\n",
      "Train Epoch: 50 [4800/48000 (10%)]\tLoss: 0.012510\n",
      "Train Epoch: 50 [9600/48000 (20%)]\tLoss: 0.023909\n",
      "Train Epoch: 50 [14400/48000 (30%)]\tLoss: 0.002669\n",
      "Train Epoch: 50 [19200/48000 (40%)]\tLoss: 0.018850\n",
      "Train Epoch: 50 [24000/48000 (50%)]\tLoss: 0.136744\n",
      "Train Epoch: 50 [28800/48000 (60%)]\tLoss: 0.005938\n",
      "Train Epoch: 50 [33600/48000 (70%)]\tLoss: 0.034836\n",
      "Train Epoch: 50 [38400/48000 (80%)]\tLoss: 0.010985\n",
      "Train Epoch: 50 [43200/48000 (90%)]\tLoss: 0.174594\n",
      "Epoch 50, Val Loss: 0.000783, Accuracy: 11906/12000 (99.22%)\n",
      "Train Epoch: 51 [0/48000 (0%)]\tLoss: 0.039650\n",
      "Train Epoch: 51 [4800/48000 (10%)]\tLoss: 0.010081\n",
      "Train Epoch: 51 [9600/48000 (20%)]\tLoss: 0.051738\n",
      "Train Epoch: 51 [14400/48000 (30%)]\tLoss: 0.011205\n",
      "Train Epoch: 51 [19200/48000 (40%)]\tLoss: 0.009108\n",
      "Train Epoch: 51 [24000/48000 (50%)]\tLoss: 0.004502\n",
      "Train Epoch: 51 [28800/48000 (60%)]\tLoss: 0.039760\n",
      "Train Epoch: 51 [33600/48000 (70%)]\tLoss: 0.007792\n",
      "Train Epoch: 51 [38400/48000 (80%)]\tLoss: 0.115241\n",
      "Train Epoch: 51 [43200/48000 (90%)]\tLoss: 0.148019\n",
      "Epoch 51, Val Loss: 0.000786, Accuracy: 11914/12000 (99.28%)\n",
      "Train Epoch: 52 [0/48000 (0%)]\tLoss: 0.005781\n",
      "Train Epoch: 52 [4800/48000 (10%)]\tLoss: 0.019005\n",
      "Train Epoch: 52 [9600/48000 (20%)]\tLoss: 0.009191\n",
      "Train Epoch: 52 [14400/48000 (30%)]\tLoss: 0.026596\n",
      "Train Epoch: 52 [19200/48000 (40%)]\tLoss: 0.266341\n",
      "Train Epoch: 52 [24000/48000 (50%)]\tLoss: 0.002074\n",
      "Train Epoch: 52 [28800/48000 (60%)]\tLoss: 0.007721\n",
      "Train Epoch: 52 [33600/48000 (70%)]\tLoss: 0.006214\n",
      "Train Epoch: 52 [38400/48000 (80%)]\tLoss: 0.009519\n",
      "Train Epoch: 52 [43200/48000 (90%)]\tLoss: 0.005523\n",
      "Epoch 52, Val Loss: 0.000803, Accuracy: 11906/12000 (99.22%)\n",
      "Train Epoch: 53 [0/48000 (0%)]\tLoss: 0.004820\n",
      "Train Epoch: 53 [4800/48000 (10%)]\tLoss: 0.070383\n",
      "Train Epoch: 53 [9600/48000 (20%)]\tLoss: 0.024692\n",
      "Train Epoch: 53 [14400/48000 (30%)]\tLoss: 0.111932\n",
      "Train Epoch: 53 [19200/48000 (40%)]\tLoss: 0.029263\n",
      "Train Epoch: 53 [24000/48000 (50%)]\tLoss: 0.001881\n",
      "Train Epoch: 53 [28800/48000 (60%)]\tLoss: 0.035098\n",
      "Train Epoch: 53 [33600/48000 (70%)]\tLoss: 0.065901\n",
      "Train Epoch: 53 [38400/48000 (80%)]\tLoss: 0.001163\n",
      "Train Epoch: 53 [43200/48000 (90%)]\tLoss: 0.030545\n",
      "Epoch 53, Val Loss: 0.000793, Accuracy: 11909/12000 (99.24%)\n",
      "Train Epoch: 54 [0/48000 (0%)]\tLoss: 0.004989\n",
      "Train Epoch: 54 [4800/48000 (10%)]\tLoss: 0.012815\n",
      "Train Epoch: 54 [9600/48000 (20%)]\tLoss: 0.012983\n",
      "Train Epoch: 54 [14400/48000 (30%)]\tLoss: 0.009268\n",
      "Train Epoch: 54 [19200/48000 (40%)]\tLoss: 0.033031\n",
      "Train Epoch: 54 [24000/48000 (50%)]\tLoss: 0.019782\n",
      "Train Epoch: 54 [28800/48000 (60%)]\tLoss: 0.000561\n",
      "Train Epoch: 54 [33600/48000 (70%)]\tLoss: 0.078991\n",
      "Train Epoch: 54 [38400/48000 (80%)]\tLoss: 0.012762\n",
      "Train Epoch: 54 [43200/48000 (90%)]\tLoss: 0.000974\n",
      "Epoch 54, Val Loss: 0.000784, Accuracy: 11908/12000 (99.23%)\n",
      "Train Epoch: 55 [0/48000 (0%)]\tLoss: 0.004477\n",
      "Train Epoch: 55 [4800/48000 (10%)]\tLoss: 0.014773\n",
      "Train Epoch: 55 [9600/48000 (20%)]\tLoss: 0.008430\n",
      "Train Epoch: 55 [14400/48000 (30%)]\tLoss: 0.078116\n",
      "Train Epoch: 55 [19200/48000 (40%)]\tLoss: 0.006652\n",
      "Train Epoch: 55 [24000/48000 (50%)]\tLoss: 0.048671\n",
      "Train Epoch: 55 [28800/48000 (60%)]\tLoss: 0.001424\n",
      "Train Epoch: 55 [33600/48000 (70%)]\tLoss: 0.087582\n",
      "Train Epoch: 55 [38400/48000 (80%)]\tLoss: 0.013490\n",
      "Train Epoch: 55 [43200/48000 (90%)]\tLoss: 0.035922\n",
      "Epoch 55, Val Loss: 0.000779, Accuracy: 11907/12000 (99.22%)\n",
      "Train Epoch: 56 [0/48000 (0%)]\tLoss: 0.004051\n",
      "Train Epoch: 56 [4800/48000 (10%)]\tLoss: 0.022624\n",
      "Train Epoch: 56 [9600/48000 (20%)]\tLoss: 0.089303\n",
      "Train Epoch: 56 [14400/48000 (30%)]\tLoss: 0.027424\n",
      "Train Epoch: 56 [19200/48000 (40%)]\tLoss: 0.026349\n",
      "Train Epoch: 56 [24000/48000 (50%)]\tLoss: 0.051595\n",
      "Train Epoch: 56 [28800/48000 (60%)]\tLoss: 0.091809\n",
      "Train Epoch: 56 [33600/48000 (70%)]\tLoss: 0.007971\n",
      "Train Epoch: 56 [38400/48000 (80%)]\tLoss: 0.005599\n",
      "Train Epoch: 56 [43200/48000 (90%)]\tLoss: 0.027058\n",
      "Epoch 56, Val Loss: 0.000777, Accuracy: 11909/12000 (99.24%)\n",
      "Train Epoch: 57 [0/48000 (0%)]\tLoss: 0.017908\n",
      "Train Epoch: 57 [4800/48000 (10%)]\tLoss: 0.010728\n",
      "Train Epoch: 57 [9600/48000 (20%)]\tLoss: 0.017439\n",
      "Train Epoch: 57 [14400/48000 (30%)]\tLoss: 0.000821\n",
      "Train Epoch: 57 [19200/48000 (40%)]\tLoss: 0.040523\n",
      "Train Epoch: 57 [24000/48000 (50%)]\tLoss: 0.072368\n",
      "Train Epoch: 57 [28800/48000 (60%)]\tLoss: 0.070806\n",
      "Train Epoch: 57 [33600/48000 (70%)]\tLoss: 0.000498\n",
      "Train Epoch: 57 [38400/48000 (80%)]\tLoss: 0.004720\n",
      "Train Epoch: 57 [43200/48000 (90%)]\tLoss: 0.012471\n",
      "Epoch 57, Val Loss: 0.000789, Accuracy: 11912/12000 (99.27%)\n",
      "Train Epoch: 58 [0/48000 (0%)]\tLoss: 0.135144\n",
      "Train Epoch: 58 [4800/48000 (10%)]\tLoss: 0.127186\n",
      "Train Epoch: 58 [9600/48000 (20%)]\tLoss: 0.100150\n",
      "Train Epoch: 58 [14400/48000 (30%)]\tLoss: 0.028056\n",
      "Train Epoch: 58 [19200/48000 (40%)]\tLoss: 0.032999\n",
      "Train Epoch: 58 [24000/48000 (50%)]\tLoss: 0.016234\n",
      "Train Epoch: 58 [28800/48000 (60%)]\tLoss: 0.001894\n",
      "Train Epoch: 58 [33600/48000 (70%)]\tLoss: 0.071778\n",
      "Train Epoch: 58 [38400/48000 (80%)]\tLoss: 0.051068\n",
      "Train Epoch: 58 [43200/48000 (90%)]\tLoss: 0.071756\n",
      "Epoch 58, Val Loss: 0.000784, Accuracy: 11918/12000 (99.32%)\n",
      "Train Epoch: 59 [0/48000 (0%)]\tLoss: 0.004172\n",
      "Train Epoch: 59 [4800/48000 (10%)]\tLoss: 0.008462\n",
      "Train Epoch: 59 [9600/48000 (20%)]\tLoss: 0.005390\n",
      "Train Epoch: 59 [14400/48000 (30%)]\tLoss: 0.025893\n",
      "Train Epoch: 59 [19200/48000 (40%)]\tLoss: 0.002237\n",
      "Train Epoch: 59 [24000/48000 (50%)]\tLoss: 0.025996\n",
      "Train Epoch: 59 [28800/48000 (60%)]\tLoss: 0.001974\n",
      "Train Epoch: 59 [33600/48000 (70%)]\tLoss: 0.200102\n",
      "Train Epoch: 59 [38400/48000 (80%)]\tLoss: 0.043354\n",
      "Train Epoch: 59 [43200/48000 (90%)]\tLoss: 0.091830\n",
      "Epoch 59, Val Loss: 0.000774, Accuracy: 11917/12000 (99.31%)\n",
      "Train Epoch: 60 [0/48000 (0%)]\tLoss: 0.073180\n",
      "Train Epoch: 60 [4800/48000 (10%)]\tLoss: 0.083127\n",
      "Train Epoch: 60 [9600/48000 (20%)]\tLoss: 0.042176\n",
      "Train Epoch: 60 [14400/48000 (30%)]\tLoss: 0.033096\n",
      "Train Epoch: 60 [19200/48000 (40%)]\tLoss: 0.009284\n",
      "Train Epoch: 60 [24000/48000 (50%)]\tLoss: 0.043577\n",
      "Train Epoch: 60 [28800/48000 (60%)]\tLoss: 0.027055\n",
      "Train Epoch: 60 [33600/48000 (70%)]\tLoss: 0.042443\n",
      "Train Epoch: 60 [38400/48000 (80%)]\tLoss: 0.034341\n",
      "Train Epoch: 60 [43200/48000 (90%)]\tLoss: 0.009578\n",
      "Epoch 60, Val Loss: 0.000778, Accuracy: 11912/12000 (99.27%)\n",
      "Train Epoch: 61 [0/48000 (0%)]\tLoss: 0.037469\n",
      "Train Epoch: 61 [4800/48000 (10%)]\tLoss: 0.017867\n",
      "Train Epoch: 61 [9600/48000 (20%)]\tLoss: 0.173940\n",
      "Train Epoch: 61 [14400/48000 (30%)]\tLoss: 0.009860\n",
      "Train Epoch: 61 [19200/48000 (40%)]\tLoss: 0.037116\n",
      "Train Epoch: 61 [24000/48000 (50%)]\tLoss: 0.008850\n",
      "Train Epoch: 61 [28800/48000 (60%)]\tLoss: 0.046597\n",
      "Train Epoch: 61 [33600/48000 (70%)]\tLoss: 0.014867\n",
      "Train Epoch: 61 [38400/48000 (80%)]\tLoss: 0.034083\n",
      "Train Epoch: 61 [43200/48000 (90%)]\tLoss: 0.043687\n",
      "Epoch 61, Val Loss: 0.000772, Accuracy: 11910/12000 (99.25%)\n",
      "Train Epoch: 62 [0/48000 (0%)]\tLoss: 0.030072\n",
      "Train Epoch: 62 [4800/48000 (10%)]\tLoss: 0.001095\n",
      "Train Epoch: 62 [9600/48000 (20%)]\tLoss: 0.019784\n",
      "Train Epoch: 62 [14400/48000 (30%)]\tLoss: 0.113839\n",
      "Train Epoch: 62 [19200/48000 (40%)]\tLoss: 0.008017\n",
      "Train Epoch: 62 [24000/48000 (50%)]\tLoss: 0.009565\n",
      "Train Epoch: 62 [28800/48000 (60%)]\tLoss: 0.084125\n",
      "Train Epoch: 62 [33600/48000 (70%)]\tLoss: 0.003851\n",
      "Train Epoch: 62 [38400/48000 (80%)]\tLoss: 0.057503\n",
      "Train Epoch: 62 [43200/48000 (90%)]\tLoss: 0.006686\n",
      "Epoch 62, Val Loss: 0.000768, Accuracy: 11911/12000 (99.26%)\n",
      "Train Epoch: 63 [0/48000 (0%)]\tLoss: 0.101757\n",
      "Train Epoch: 63 [4800/48000 (10%)]\tLoss: 0.066269\n",
      "Train Epoch: 63 [9600/48000 (20%)]\tLoss: 0.034634\n",
      "Train Epoch: 63 [14400/48000 (30%)]\tLoss: 0.026592\n",
      "Train Epoch: 63 [19200/48000 (40%)]\tLoss: 0.011281\n",
      "Train Epoch: 63 [24000/48000 (50%)]\tLoss: 0.022140\n",
      "Train Epoch: 63 [28800/48000 (60%)]\tLoss: 0.009445\n",
      "Train Epoch: 63 [33600/48000 (70%)]\tLoss: 0.002650\n",
      "Train Epoch: 63 [38400/48000 (80%)]\tLoss: 0.007820\n",
      "Train Epoch: 63 [43200/48000 (90%)]\tLoss: 0.003818\n",
      "Epoch 63, Val Loss: 0.000784, Accuracy: 11913/12000 (99.28%)\n",
      "Train Epoch: 64 [0/48000 (0%)]\tLoss: 0.022784\n",
      "Train Epoch: 64 [4800/48000 (10%)]\tLoss: 0.122821\n",
      "Train Epoch: 64 [9600/48000 (20%)]\tLoss: 0.031467\n",
      "Train Epoch: 64 [14400/48000 (30%)]\tLoss: 0.020513\n",
      "Train Epoch: 64 [19200/48000 (40%)]\tLoss: 0.059342\n",
      "Train Epoch: 64 [24000/48000 (50%)]\tLoss: 0.036700\n",
      "Train Epoch: 64 [28800/48000 (60%)]\tLoss: 0.015165\n",
      "Train Epoch: 64 [33600/48000 (70%)]\tLoss: 0.008830\n",
      "Train Epoch: 64 [38400/48000 (80%)]\tLoss: 0.020680\n",
      "Train Epoch: 64 [43200/48000 (90%)]\tLoss: 0.002216\n",
      "Epoch 64, Val Loss: 0.000792, Accuracy: 11907/12000 (99.22%)\n",
      "Train Epoch: 65 [0/48000 (0%)]\tLoss: 0.006520\n",
      "Train Epoch: 65 [4800/48000 (10%)]\tLoss: 0.030664\n",
      "Train Epoch: 65 [9600/48000 (20%)]\tLoss: 0.002554\n",
      "Train Epoch: 65 [14400/48000 (30%)]\tLoss: 0.046022\n",
      "Train Epoch: 65 [19200/48000 (40%)]\tLoss: 0.011828\n",
      "Train Epoch: 65 [24000/48000 (50%)]\tLoss: 0.054114\n",
      "Train Epoch: 65 [28800/48000 (60%)]\tLoss: 0.001846\n",
      "Train Epoch: 65 [33600/48000 (70%)]\tLoss: 0.041768\n",
      "Train Epoch: 65 [38400/48000 (80%)]\tLoss: 0.035295\n",
      "Train Epoch: 65 [43200/48000 (90%)]\tLoss: 0.008021\n",
      "Epoch 65, Val Loss: 0.000774, Accuracy: 11912/12000 (99.27%)\n",
      "Train Epoch: 66 [0/48000 (0%)]\tLoss: 0.059019\n",
      "Train Epoch: 66 [4800/48000 (10%)]\tLoss: 0.004097\n",
      "Train Epoch: 66 [9600/48000 (20%)]\tLoss: 0.060011\n",
      "Train Epoch: 66 [14400/48000 (30%)]\tLoss: 0.006769\n",
      "Train Epoch: 66 [19200/48000 (40%)]\tLoss: 0.021414\n",
      "Train Epoch: 66 [24000/48000 (50%)]\tLoss: 0.010009\n",
      "Train Epoch: 66 [28800/48000 (60%)]\tLoss: 0.032543\n",
      "Train Epoch: 66 [33600/48000 (70%)]\tLoss: 0.011576\n",
      "Train Epoch: 66 [38400/48000 (80%)]\tLoss: 0.005030\n",
      "Train Epoch: 66 [43200/48000 (90%)]\tLoss: 0.002849\n",
      "Epoch 66, Val Loss: 0.000777, Accuracy: 11919/12000 (99.33%)\n",
      "Train Epoch: 67 [0/48000 (0%)]\tLoss: 0.011542\n",
      "Train Epoch: 67 [4800/48000 (10%)]\tLoss: 0.011092\n",
      "Train Epoch: 67 [9600/48000 (20%)]\tLoss: 0.000680\n",
      "Train Epoch: 67 [14400/48000 (30%)]\tLoss: 0.000815\n",
      "Train Epoch: 67 [19200/48000 (40%)]\tLoss: 0.077783\n",
      "Train Epoch: 67 [24000/48000 (50%)]\tLoss: 0.037603\n",
      "Train Epoch: 67 [28800/48000 (60%)]\tLoss: 0.008586\n",
      "Train Epoch: 67 [33600/48000 (70%)]\tLoss: 0.002795\n",
      "Train Epoch: 67 [38400/48000 (80%)]\tLoss: 0.185870\n",
      "Train Epoch: 67 [43200/48000 (90%)]\tLoss: 0.004133\n",
      "Epoch 67, Val Loss: 0.000771, Accuracy: 11917/12000 (99.31%)\n",
      "Early stopping at epoch 67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "num_epochs = 100  # Set a high number of epochs\n",
    "\n",
    "#optimizer = optim.Adam(module.parameters(), lr=0.00001, weight_decay=0)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    module.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        step += 1\n",
    "\n",
    "        #step = batch_idx + len(train_loader)*epoch+1\n",
    "\n",
    "        labels = labels.view(-1, num_heads)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = module(images, 'training')\n",
    "\n",
    "\n",
    "        total_loss = 0\n",
    "        for i in range(num_heads):\n",
    "            total_loss += criterion(predictions[:, i, :], labels[:, i])\n",
    "        total_loss /= num_heads\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = total_loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(images)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss:.6f}\")\n",
    "\n",
    "    module.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            predictions = module(images, 'inference')\n",
    "            \n",
    "            mimo_output_avg = predictions.mean(dim=1)\n",
    "            val_loss += criterion(mimo_output_avg, labels).item()\n",
    "            _, predicted = torch.max(mimo_output_avg, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Epoch {epoch}, Val Loss: {val_loss:.6f}, Accuracy: {correct}/{len(val_loader.dataset)} ({accuracy:.2f}%)')\n",
    "\n",
    "    '''\n",
    "    wandb.log({\n",
    "        \"Iter step\": step,\n",
    "        \"valid/accs\": accuracy,\n",
    "        #\"test/accs\": test_accs_b,\n",
    "        #\"valid/best_acc\": self.best_valid_acc_b,\n",
    "        #\"test/best_acc\": self.best_test_acc_b,\n",
    "    })\n",
    "    '''\n",
    "\n",
    "    # Check for early stopping based on test loss\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = module.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "module.load_state_dict(best_model_state)\n",
    "\n",
    "#wandb_switch('Multi output module', 'First run', 0, 'finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 Train set:\n",
      "Average Entropy for Base Model Outputs: 0.0235\n",
      "Average Entropy for Multi Outputs: 0.0230\n",
      "CIFAR10 Test set:\n",
      "Average Entropy for Base Model Outputs: 0.0258\n",
      "Average Entropy for Multi Outputs: 0.0244\n",
      "FashionMNIST OOD set:\n",
      "Average Entropy for Base Model Outputs: 0.4406\n",
      "Average Entropy for Multi Outputs: 0.5334\n",
      "CIFAR100 OOD set:\n",
      "Average Entropy for Base Model Outputs: 0.6094\n",
      "Average Entropy for Multi Outputs: 0.7213\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to compute and print the mean entropy of softmax outputs for base_model and module\n",
    "def compute_mean_entropy_of_mean_softmax(loader, base_model, module, device):\n",
    "    base_model.eval()\n",
    "    module.eval()\n",
    "\n",
    "    total_base_entropy_sum = 0\n",
    "    total_multi_output_entropy_sum = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "            # Base model prediction and entropy calculation\n",
    "            base_outputs = base_model(images)\n",
    "            base_output_softmax = F.softmax(base_outputs, dim=1).cpu().numpy()\n",
    "            base_entropies = np.sum(entropy(base_output_softmax, axis=1))\n",
    "            total_base_entropy_sum += base_entropies\n",
    "\n",
    "            # Repeat and reshape images for the multi-output module\n",
    "            multi_output_predictions = module(images, 'inference')\n",
    "            multi_output_avg = multi_output_predictions.mean(dim=1).cpu().numpy()\n",
    "            multi_output_softmax = F.softmax(torch.tensor(multi_output_avg), dim=1).numpy()\n",
    "            multi_output_entropy = np.sum(entropy(multi_output_softmax, axis=1))\n",
    "            total_multi_output_entropy_sum += multi_output_entropy\n",
    "\n",
    "    average_base_entropy = total_base_entropy_sum / num_samples\n",
    "    average_multi_output_entropy = total_multi_output_entropy_sum / num_samples\n",
    "\n",
    "    print(f\"Average Entropy for Base Model Outputs: {average_base_entropy:.4f}\")\n",
    "    print(f\"Average Entropy for Multi Outputs: {average_multi_output_entropy:.4f}\")\n",
    "\n",
    "    return average_base_entropy, average_multi_output_entropy\n",
    "\n",
    "\n",
    "print(\"CIFAR10 Train set:\")\n",
    "train_base_entropy, train_multi_output_entropy = compute_mean_entropy_of_mean_softmax(train_loader, base_model, module, device)\n",
    "print(\"CIFAR10 Test set:\")\n",
    "test_base_entropy, test_multi_output_entropy = compute_mean_entropy_of_mean_softmax(test_loader, base_model, module, device)\n",
    "print(\"FashionMNIST OOD set:\")\n",
    "fashion_base_entropy, fashion_multi_output_entropy = compute_mean_entropy_of_mean_softmax(fashion_ood_loader, base_model, module, device)\n",
    "print(\"CIFAR100 OOD set:\")\n",
    "cifar100_base_entropy, cifar100_multi_output_entropy = compute_mean_entropy_of_mean_softmax(cifar100_ood_loader, base_model, module, device)\n",
    "\n",
    "labels = ['CIFAR10 Train', 'CIFAR10 Test', 'FashionMNIST OOD', 'CIFAR100 OOD']\n",
    "base_entropies = [train_base_entropy, test_base_entropy, fashion_base_entropy, cifar100_base_entropy]\n",
    "multi_output_entropies = [train_multi_output_entropy, test_multi_output_entropy, fashion_multi_output_entropy, cifar100_multi_output_entropy]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, base_entropies, width, label='Base Model')\n",
    "rects2 = ax.bar(x + width/2, multi_output_entropies, width, label='Multi-Output Module')\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Average Entropy of Softmax results')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy: 98.49%, Brier Score: 0.0023, NLL: 0.0517, ECE: 0.0060, MCE: 0.6878\n",
      "Multi-Output Module Accuracy: 98.68%, Brier Score: 0.0020, NLL: 0.0388, ECE: 0.0009, MCE: 0.3636\n"
     ]
    }
   ],
   "source": [
    "def compute_mce(predictions, confidences, labels, n_bins=15):\n",
    "    \"\"\"\n",
    "    Compute Maximum Calibration Error (MCE).\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions (list or np.array): Model's predicted labels.\n",
    "    - confidences (list or np.array): Model's confidence scores.\n",
    "    - labels (list or np.array): True labels.\n",
    "    - n_bins (int): Number of bins to compute calibration error.\n",
    "    \n",
    "    Returns:\n",
    "    - mce (float): Maximum Calibration Error.\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    mce = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        if np.any(in_bin):\n",
    "            accuracy_in_bin = np.mean(predictions[in_bin] == labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            mce = max(mce, np.abs(avg_confidence_in_bin - accuracy_in_bin))\n",
    "    \n",
    "    return mce\n",
    "\n",
    "def compute_calibration_metrics(loader, base_model, module, device, num_classes=10):\n",
    "    \"\"\"\n",
    "    Compute accuracy and calibration metrics for both base model and multi-output module.\n",
    "    \n",
    "    Parameters:\n",
    "    - loader (DataLoader): Test data loader.\n",
    "    - base_model (nn.Module): Base neural network model.\n",
    "    - module (nn.Module): Multi-output module for calibration.\n",
    "    - device (torch.device): Device to perform computations on.\n",
    "    - num_classes (int): Number of output classes.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing accuracy and calibration metrics for both models.\n",
    "    \"\"\"\n",
    "    base_model.eval()\n",
    "    module.eval()\n",
    "\n",
    "    base_ece_metric = CalibrationError(n_bins=15, task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "    multi_output_ece_metric = CalibrationError(n_bins=15, task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "\n",
    "    base_all_labels, base_all_confidences, base_all_predictions = [], [], []\n",
    "    multi_output_all_labels, multi_output_all_confidences, multi_output_all_predictions = [], [], []\n",
    "\n",
    "    base_correct, base_total, base_test_loss = 0, 0, 0.0\n",
    "    multi_output_correct, multi_output_total, multi_output_test_loss = 0, 0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            base_outputs = base_model(images)\n",
    "            base_test_loss += F.cross_entropy(base_outputs, labels, reduction='sum').item()\n",
    "            base_preds = base_outputs.argmax(dim=1)\n",
    "            base_correct += base_preds.eq(labels).sum().item()\n",
    "            base_total += labels.size(0)\n",
    "\n",
    "            base_softmax_outputs = F.softmax(base_outputs, dim=1)\n",
    "            base_ece_metric.update(base_softmax_outputs, labels)\n",
    "\n",
    "            base_all_labels.extend(labels.cpu().numpy())\n",
    "            base_all_confidences.extend(base_softmax_outputs.cpu().numpy())  # Store full softmax outputs\n",
    "            base_all_predictions.extend(base_preds.cpu().numpy())\n",
    "\n",
    "            multi_output_predictions = module(images, 'inference')\n",
    "            multi_output_softmax = F.softmax(multi_output_predictions, dim=-1)\n",
    "            multi_output_avg = multi_output_softmax.mean(dim=1)\n",
    "            multi_output_test_loss += F.cross_entropy(multi_output_avg, labels, reduction='sum').item()\n",
    "            multi_output_preds = multi_output_avg.argmax(dim=1)\n",
    "            multi_output_correct += multi_output_preds.eq(labels).sum().item()\n",
    "            multi_output_total += labels.size(0)\n",
    "\n",
    "            multi_output_ece_metric.update(multi_output_avg, labels)\n",
    "\n",
    "            multi_output_all_labels.extend(labels.cpu().numpy())\n",
    "            multi_output_all_confidences.extend(multi_output_avg.cpu().numpy())  # Store full softmax outputs\n",
    "            multi_output_all_predictions.extend(multi_output_preds.cpu().numpy())\n",
    "\n",
    "    base_all_confidences = np.array(base_all_confidences)  # Ensure this is 2D\n",
    "    multi_output_all_confidences = np.array(multi_output_all_confidences)  # Ensure this is 2D\n",
    "\n",
    "    base_accuracy = 100. * base_correct / base_total\n",
    "    base_test_loss /= base_total\n",
    "    base_ece = base_ece_metric.compute().item()\n",
    "\n",
    "    base_nll = log_loss(base_all_labels, base_all_confidences, labels=np.arange(num_classes))\n",
    "    base_brier = mean_squared_error(F.one_hot(torch.tensor(base_all_labels), num_classes=num_classes).numpy(), base_all_confidences)\n",
    "    base_mce = compute_mce(np.array(base_all_predictions), base_all_confidences.max(axis=1), np.array(base_all_labels))\n",
    "\n",
    "    multi_output_accuracy = 100. * multi_output_correct / multi_output_total\n",
    "    multi_output_test_loss /= multi_output_total\n",
    "    multi_output_ece = multi_output_ece_metric.compute().item()\n",
    "\n",
    "    multi_output_nll = log_loss(multi_output_all_labels, multi_output_all_confidences, labels=np.arange(num_classes))\n",
    "    multi_output_brier = mean_squared_error(F.one_hot(torch.tensor(multi_output_all_labels), num_classes=num_classes).numpy(), multi_output_all_confidences)\n",
    "    multi_output_mce = compute_mce(np.array(multi_output_all_predictions), multi_output_all_confidences.max(axis=1), np.array(multi_output_all_labels))\n",
    "\n",
    "    print(f\"Base Model Accuracy: {base_accuracy:.2f}%, Brier Score: {base_brier:.4f}, NLL: {base_nll:.4f}, ECE: {base_ece:.4f}, MCE: {base_mce:.4f}\")\n",
    "    print(f\"Multi-Output Module Accuracy: {multi_output_accuracy:.2f}%, Brier Score: {multi_output_brier:.4f}, NLL: {multi_output_nll:.4f}, ECE: {multi_output_ece:.4f}, MCE: {multi_output_mce:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"Base Model\": {\n",
    "            \"Test NLL\": base_nll,\n",
    "            \"Test Accuracy\": base_accuracy,\n",
    "            \"Test Cal. Error\": base_ece,\n",
    "            \"Test Brier Score\": base_brier,\n",
    "            \"Test MCE\": base_mce,\n",
    "        },\n",
    "        \"Multi-Output Module\": {\n",
    "            \"Test NLL\": multi_output_nll,\n",
    "            \"Test Accuracy\": multi_output_accuracy,\n",
    "            \"Test Cal. Error\": multi_output_ece,\n",
    "            \"Test Brier Score\": multi_output_brier,\n",
    "            \"Test MCE\": multi_output_mce,\n",
    "        }\n",
    "    }\n",
    "\n",
    "accuracy_metrics = compute_calibration_metrics(test_loader, base_model, module, device, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
