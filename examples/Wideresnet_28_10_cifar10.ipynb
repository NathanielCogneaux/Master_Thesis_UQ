{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import entropy\n",
    "from sklearn.metrics import log_loss, roc_auc_score, mean_squared_error, precision_recall_curve, auc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchmetrics import CalibrationError\n",
    "\n",
    "from models.wide_resnet import Wide_ResNet\n",
    "from multi_output_module.multi_output_module import Multi_output_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSeverityAugMix:\n",
    "    def __init__(self, min_severity=1, max_severity=5, prob=0.7, mixture_width=3, chain_depth=-1, alpha=1.0, all_ops=True, interpolation=InterpolationMode.BILINEAR, fill=None):\n",
    "        self.min_severity = min_severity\n",
    "        self.max_severity = max_severity\n",
    "        self.prob = prob  # Probability of applying AugMix\n",
    "        self.mixture_width = mixture_width\n",
    "        self.chain_depth = chain_depth\n",
    "        self.alpha = alpha\n",
    "        self.all_ops = all_ops\n",
    "        self.interpolation = interpolation\n",
    "        self.fill = fill\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Randomly decide whether to apply AugMix based on the probability\n",
    "        if random.random() < self.prob:\n",
    "            # If chosen to apply AugMix, randomly choose severity\n",
    "            severity = random.randint(self.min_severity, self.max_severity)\n",
    "            \n",
    "            # Apply AugMix transformation\n",
    "            augmix_transform = transforms.AugMix(\n",
    "                severity=severity,\n",
    "                mixture_width=self.mixture_width,\n",
    "                chain_depth=self.chain_depth,\n",
    "                alpha=self.alpha,\n",
    "                all_ops=self.all_ops,\n",
    "                interpolation=self.interpolation,\n",
    "                fill=self.fill\n",
    "            )\n",
    "            \n",
    "            # Apply the AugMix transformation to the image\n",
    "            img = augmix_transform(img)\n",
    "        \n",
    "        # If not applying AugMix, return the original image\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = '/home/natcgx/Single-pass UQ research project/data'\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "\n",
    "# Define the transformations for the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "'''\n",
    "val_transform_cifar10 = transforms.Compose([\n",
    "    RandomSeverityAugMix(min_severity=1, max_severity=10, prob=0.8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "'''\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True, download=False, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False, download=False, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "#train_dataset.transform = val_transform_cifar10\n",
    "#val_dataset.dataset.transform = val_transform_cifar10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the transformations for the FashionMNIST dataset, converting grayscale to RGB\n",
    "transform_fashion = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel RGB\n",
    "    transforms.Resize((32, 32)),  # Resize to 32x32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizing for RGB\n",
    "])\n",
    "\n",
    "fashion_ood_dataset = torchvision.datasets.FashionMNIST(root=data_path, train=False, download=True, transform=transform_fashion)\n",
    "\n",
    "fashion_ood_loader = DataLoader(fashion_ood_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the transformations for the CIFAR-100 dataset\n",
    "transform_cifar100 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])\n",
    "\n",
    "cifar100_ood_dataset = torchvision.datasets.CIFAR100(root=data_path, train=False, download=True, transform=transform_cifar100)\n",
    "\n",
    "cifar100_ood_loader = DataLoader(cifar100_ood_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Wide_ResNet(depth=28, widen_factor=10, dropout_rate=0.3, num_classes=num_classes)\n",
    "base_model.load_state_dict(torch.load('./models/saved_models/cifar/cifar10/wide-resnet-28x10.pth'))\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.1574, Accuracy: 9634/10000 (96.34%)\n",
      "\n",
      "Reloaded model test accuracy: 96.34%\n"
     ]
    }
   ],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test(base_model, device, test_loader)\n",
    "print(f'Reloaded model test accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 20\n",
    "batch_size = 16\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi_output_module(\n",
      "  (activation): ReLU()\n",
      "  (base_model): WideResNet(\n",
      "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (block1): NetworkBlock(\n",
      "      (layer): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (block2): NetworkBlock(\n",
      "      (layer): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (block3): NetworkBlock(\n",
      "      (layer): Sequential(\n",
      "        (0): BasicBlock(\n",
      "          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        )\n",
      "        (1): BasicBlock(\n",
      "          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): BasicBlock(\n",
      "          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): BasicBlock(\n",
      "          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (fc): Linear(in_features=640, out_features=10, bias=True)\n",
      "  )\n",
      "  (last_layer): Linear(in_features=640, out_features=10, bias=True)\n",
      "  (input_heads): ModuleList(\n",
      "    (0-19): 20 x Linear(in_features=640, out_features=10, bias=True)\n",
      "  )\n",
      "  (shared_layers): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (output_layers): ModuleList(\n",
      "    (0-19): 20 x Linear(in_features=200, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_model.eval()\n",
    "\n",
    "module = Multi_output_module(num_heads, base_model, device).to(device)\n",
    "print(module)\n",
    "\n",
    "optimizer = optim.Adam(module.parameters(), lr=0.0001, weight_decay=0.0005)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = num_heads * batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 36687794\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in module.parameters())\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/45000 (0%)]\tLoss: 2.440709\n",
      "Train Epoch: 0 [32000/45000 (71%)]\tLoss: 2.280950\n",
      "Epoch 0, Val Loss: 0.055950, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 1 [0/45000 (0%)]\tLoss: 2.257169\n",
      "Train Epoch: 1 [32000/45000 (71%)]\tLoss: 2.168582\n",
      "Epoch 1, Val Loss: 0.052576, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 2 [0/45000 (0%)]\tLoss: 2.162543\n",
      "Train Epoch: 2 [32000/45000 (71%)]\tLoss: 2.029727\n",
      "Epoch 2, Val Loss: 0.047037, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 3 [0/45000 (0%)]\tLoss: 1.965515\n",
      "Train Epoch: 3 [32000/45000 (71%)]\tLoss: 1.848817\n",
      "Epoch 3, Val Loss: 0.041499, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 4 [0/45000 (0%)]\tLoss: 1.816037\n",
      "Train Epoch: 4 [32000/45000 (71%)]\tLoss: 1.681634\n",
      "Epoch 4, Val Loss: 0.035895, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 5 [0/45000 (0%)]\tLoss: 1.669050\n",
      "Train Epoch: 5 [32000/45000 (71%)]\tLoss: 1.507789\n",
      "Epoch 5, Val Loss: 0.030446, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 6 [0/45000 (0%)]\tLoss: 1.404009\n",
      "Train Epoch: 6 [32000/45000 (71%)]\tLoss: 1.304882\n",
      "Epoch 6, Val Loss: 0.025084, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 7 [0/45000 (0%)]\tLoss: 1.277998\n",
      "Train Epoch: 7 [32000/45000 (71%)]\tLoss: 1.215497\n",
      "Epoch 7, Val Loss: 0.020592, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 8 [0/45000 (0%)]\tLoss: 1.195840\n",
      "Train Epoch: 8 [32000/45000 (71%)]\tLoss: 1.027140\n",
      "Epoch 8, Val Loss: 0.016737, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 9 [0/45000 (0%)]\tLoss: 0.998940\n",
      "Train Epoch: 9 [32000/45000 (71%)]\tLoss: 0.910020\n",
      "Epoch 9, Val Loss: 0.013487, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 10 [0/45000 (0%)]\tLoss: 0.885658\n",
      "Train Epoch: 10 [32000/45000 (71%)]\tLoss: 0.836658\n",
      "Epoch 10, Val Loss: 0.010861, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 11 [0/45000 (0%)]\tLoss: 0.765430\n",
      "Train Epoch: 11 [32000/45000 (71%)]\tLoss: 0.712334\n",
      "Epoch 11, Val Loss: 0.008807, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 12 [0/45000 (0%)]\tLoss: 0.729996\n",
      "Train Epoch: 12 [32000/45000 (71%)]\tLoss: 0.624241\n",
      "Epoch 12, Val Loss: 0.007099, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 13 [0/45000 (0%)]\tLoss: 0.629958\n",
      "Train Epoch: 13 [32000/45000 (71%)]\tLoss: 0.554325\n",
      "Epoch 13, Val Loss: 0.005726, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 14 [0/45000 (0%)]\tLoss: 0.533135\n",
      "Train Epoch: 14 [32000/45000 (71%)]\tLoss: 0.485730\n",
      "Epoch 14, Val Loss: 0.004712, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 15 [0/45000 (0%)]\tLoss: 0.456868\n",
      "Train Epoch: 15 [32000/45000 (71%)]\tLoss: 0.432156\n",
      "Epoch 15, Val Loss: 0.003898, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 16 [0/45000 (0%)]\tLoss: 0.411113\n",
      "Train Epoch: 16 [32000/45000 (71%)]\tLoss: 0.364430\n",
      "Epoch 16, Val Loss: 0.003256, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 17 [0/45000 (0%)]\tLoss: 0.357952\n",
      "Train Epoch: 17 [32000/45000 (71%)]\tLoss: 0.330268\n",
      "Epoch 17, Val Loss: 0.002719, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 18 [0/45000 (0%)]\tLoss: 0.298894\n",
      "Train Epoch: 18 [32000/45000 (71%)]\tLoss: 0.330931\n",
      "Epoch 18, Val Loss: 0.002324, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 19 [0/45000 (0%)]\tLoss: 0.272788\n",
      "Train Epoch: 19 [32000/45000 (71%)]\tLoss: 0.262134\n",
      "Epoch 19, Val Loss: 0.001989, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 20 [0/45000 (0%)]\tLoss: 0.238506\n",
      "Train Epoch: 20 [32000/45000 (71%)]\tLoss: 0.226799\n",
      "Epoch 20, Val Loss: 0.001719, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 21 [0/45000 (0%)]\tLoss: 0.232114\n",
      "Train Epoch: 21 [32000/45000 (71%)]\tLoss: 0.222092\n",
      "Epoch 21, Val Loss: 0.001505, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 22 [0/45000 (0%)]\tLoss: 0.202007\n",
      "Train Epoch: 22 [32000/45000 (71%)]\tLoss: 0.181131\n",
      "Epoch 22, Val Loss: 0.001329, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 23 [0/45000 (0%)]\tLoss: 0.177282\n",
      "Train Epoch: 23 [32000/45000 (71%)]\tLoss: 0.179103\n",
      "Epoch 23, Val Loss: 0.001185, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 24 [0/45000 (0%)]\tLoss: 0.150757\n",
      "Train Epoch: 24 [32000/45000 (71%)]\tLoss: 0.153033\n",
      "Epoch 24, Val Loss: 0.001065, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 25 [0/45000 (0%)]\tLoss: 0.137820\n",
      "Train Epoch: 25 [32000/45000 (71%)]\tLoss: 0.140887\n",
      "Epoch 25, Val Loss: 0.000956, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 26 [0/45000 (0%)]\tLoss: 0.140590\n",
      "Train Epoch: 26 [32000/45000 (71%)]\tLoss: 0.129210\n",
      "Epoch 26, Val Loss: 0.000871, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 27 [0/45000 (0%)]\tLoss: 0.130904\n",
      "Train Epoch: 27 [32000/45000 (71%)]\tLoss: 0.122265\n",
      "Epoch 27, Val Loss: 0.000801, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 28 [0/45000 (0%)]\tLoss: 0.114367\n",
      "Train Epoch: 28 [32000/45000 (71%)]\tLoss: 0.105723\n",
      "Epoch 28, Val Loss: 0.000736, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 29 [0/45000 (0%)]\tLoss: 0.109061\n",
      "Train Epoch: 29 [32000/45000 (71%)]\tLoss: 0.097348\n",
      "Epoch 29, Val Loss: 0.000688, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 30 [0/45000 (0%)]\tLoss: 0.090893\n",
      "Train Epoch: 30 [32000/45000 (71%)]\tLoss: 0.101152\n",
      "Epoch 30, Val Loss: 0.000642, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 31 [0/45000 (0%)]\tLoss: 0.084671\n",
      "Train Epoch: 31 [32000/45000 (71%)]\tLoss: 0.091923\n",
      "Epoch 31, Val Loss: 0.000604, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 32 [0/45000 (0%)]\tLoss: 0.082652\n",
      "Train Epoch: 32 [32000/45000 (71%)]\tLoss: 0.079419\n",
      "Epoch 32, Val Loss: 0.000570, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 33 [0/45000 (0%)]\tLoss: 0.080746\n",
      "Train Epoch: 33 [32000/45000 (71%)]\tLoss: 0.073948\n",
      "Epoch 33, Val Loss: 0.000542, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 34 [0/45000 (0%)]\tLoss: 0.072163\n",
      "Train Epoch: 34 [32000/45000 (71%)]\tLoss: 0.076808\n",
      "Epoch 34, Val Loss: 0.000517, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 35 [0/45000 (0%)]\tLoss: 0.071611\n",
      "Train Epoch: 35 [32000/45000 (71%)]\tLoss: 0.058694\n",
      "Epoch 35, Val Loss: 0.000500, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 36 [0/45000 (0%)]\tLoss: 0.069744\n",
      "Train Epoch: 36 [32000/45000 (71%)]\tLoss: 0.061599\n",
      "Epoch 36, Val Loss: 0.000480, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 37 [0/45000 (0%)]\tLoss: 0.068621\n",
      "Train Epoch: 37 [32000/45000 (71%)]\tLoss: 0.060845\n",
      "Epoch 37, Val Loss: 0.000465, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 38 [0/45000 (0%)]\tLoss: 0.067607\n",
      "Train Epoch: 38 [32000/45000 (71%)]\tLoss: 0.062654\n",
      "Epoch 38, Val Loss: 0.000450, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 39 [0/45000 (0%)]\tLoss: 0.055467\n",
      "Train Epoch: 39 [32000/45000 (71%)]\tLoss: 0.055730\n",
      "Epoch 39, Val Loss: 0.000438, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 40 [0/45000 (0%)]\tLoss: 0.059293\n",
      "Train Epoch: 40 [32000/45000 (71%)]\tLoss: 0.048243\n",
      "Epoch 40, Val Loss: 0.000427, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 41 [0/45000 (0%)]\tLoss: 0.051808\n",
      "Train Epoch: 41 [32000/45000 (71%)]\tLoss: 0.048671\n",
      "Epoch 41, Val Loss: 0.000418, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 42 [0/45000 (0%)]\tLoss: 0.049809\n",
      "Train Epoch: 42 [32000/45000 (71%)]\tLoss: 0.049432\n",
      "Epoch 42, Val Loss: 0.000409, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 43 [0/45000 (0%)]\tLoss: 0.052286\n",
      "Train Epoch: 43 [32000/45000 (71%)]\tLoss: 0.046912\n",
      "Epoch 43, Val Loss: 0.000400, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 44 [0/45000 (0%)]\tLoss: 0.041853\n",
      "Train Epoch: 44 [32000/45000 (71%)]\tLoss: 0.048898\n",
      "Epoch 44, Val Loss: 0.000394, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 45 [0/45000 (0%)]\tLoss: 0.044319\n",
      "Train Epoch: 45 [32000/45000 (71%)]\tLoss: 0.041472\n",
      "Epoch 45, Val Loss: 0.000389, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 46 [0/45000 (0%)]\tLoss: 0.054574\n",
      "Train Epoch: 46 [32000/45000 (71%)]\tLoss: 0.045809\n",
      "Epoch 46, Val Loss: 0.000384, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 47 [0/45000 (0%)]\tLoss: 0.040768\n",
      "Train Epoch: 47 [32000/45000 (71%)]\tLoss: 0.040002\n",
      "Epoch 47, Val Loss: 0.000381, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 48 [0/45000 (0%)]\tLoss: 0.040189\n",
      "Train Epoch: 48 [32000/45000 (71%)]\tLoss: 0.041722\n",
      "Epoch 48, Val Loss: 0.000378, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 49 [0/45000 (0%)]\tLoss: 0.042816\n",
      "Train Epoch: 49 [32000/45000 (71%)]\tLoss: 0.041812\n",
      "Epoch 49, Val Loss: 0.000376, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 50 [0/45000 (0%)]\tLoss: 0.041972\n",
      "Train Epoch: 50 [32000/45000 (71%)]\tLoss: 0.041676\n",
      "Epoch 50, Val Loss: 0.000372, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 51 [0/45000 (0%)]\tLoss: 0.036525\n",
      "Train Epoch: 51 [32000/45000 (71%)]\tLoss: 0.037782\n",
      "Epoch 51, Val Loss: 0.000368, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 52 [0/45000 (0%)]\tLoss: 0.043273\n",
      "Train Epoch: 52 [32000/45000 (71%)]\tLoss: 0.039395\n",
      "Epoch 52, Val Loss: 0.000368, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 53 [0/45000 (0%)]\tLoss: 0.036160\n",
      "Train Epoch: 53 [32000/45000 (71%)]\tLoss: 0.037434\n",
      "Epoch 53, Val Loss: 0.000365, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 54 [0/45000 (0%)]\tLoss: 0.035550\n",
      "Train Epoch: 54 [32000/45000 (71%)]\tLoss: 0.036382\n",
      "Epoch 54, Val Loss: 0.000362, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 55 [0/45000 (0%)]\tLoss: 0.030689\n",
      "Train Epoch: 55 [32000/45000 (71%)]\tLoss: 0.033216\n",
      "Epoch 55, Val Loss: 0.000359, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 56 [0/45000 (0%)]\tLoss: 0.037401\n",
      "Train Epoch: 56 [32000/45000 (71%)]\tLoss: 0.036757\n",
      "Epoch 56, Val Loss: 0.000355, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 57 [0/45000 (0%)]\tLoss: 0.032889\n",
      "Train Epoch: 57 [32000/45000 (71%)]\tLoss: 0.032838\n",
      "Epoch 57, Val Loss: 0.000355, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 58 [0/45000 (0%)]\tLoss: 0.032129\n",
      "Train Epoch: 58 [32000/45000 (71%)]\tLoss: 0.037936\n",
      "Epoch 58, Val Loss: 0.000352, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 59 [0/45000 (0%)]\tLoss: 0.033650\n",
      "Train Epoch: 59 [32000/45000 (71%)]\tLoss: 0.030276\n",
      "Epoch 59, Val Loss: 0.000349, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 60 [0/45000 (0%)]\tLoss: 0.031587\n",
      "Train Epoch: 60 [32000/45000 (71%)]\tLoss: 0.029428\n",
      "Epoch 60, Val Loss: 0.000349, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 61 [0/45000 (0%)]\tLoss: 0.034823\n",
      "Train Epoch: 61 [32000/45000 (71%)]\tLoss: 0.031160\n",
      "Epoch 61, Val Loss: 0.000349, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 62 [0/45000 (0%)]\tLoss: 0.030730\n",
      "Train Epoch: 62 [32000/45000 (71%)]\tLoss: 0.030155\n",
      "Epoch 62, Val Loss: 0.000347, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 63 [0/45000 (0%)]\tLoss: 0.029918\n",
      "Train Epoch: 63 [32000/45000 (71%)]\tLoss: 0.031173\n",
      "Epoch 63, Val Loss: 0.000345, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 64 [0/45000 (0%)]\tLoss: 0.030316\n",
      "Train Epoch: 64 [32000/45000 (71%)]\tLoss: 0.032228\n",
      "Epoch 64, Val Loss: 0.000346, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 65 [0/45000 (0%)]\tLoss: 0.033676\n",
      "Train Epoch: 65 [32000/45000 (71%)]\tLoss: 0.029779\n",
      "Epoch 65, Val Loss: 0.000344, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 66 [0/45000 (0%)]\tLoss: 0.030634\n",
      "Train Epoch: 66 [32000/45000 (71%)]\tLoss: 0.028094\n",
      "Epoch 66, Val Loss: 0.000344, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 67 [0/45000 (0%)]\tLoss: 0.031568\n",
      "Train Epoch: 67 [32000/45000 (71%)]\tLoss: 0.030367\n",
      "Epoch 67, Val Loss: 0.000343, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 68 [0/45000 (0%)]\tLoss: 0.029188\n",
      "Train Epoch: 68 [32000/45000 (71%)]\tLoss: 0.028786\n",
      "Epoch 68, Val Loss: 0.000343, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 69 [0/45000 (0%)]\tLoss: 0.028941\n",
      "Train Epoch: 69 [32000/45000 (71%)]\tLoss: 0.028967\n",
      "Epoch 69, Val Loss: 0.000341, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 70 [0/45000 (0%)]\tLoss: 0.025593\n",
      "Train Epoch: 70 [32000/45000 (71%)]\tLoss: 0.028437\n",
      "Epoch 70, Val Loss: 0.000341, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 71 [0/45000 (0%)]\tLoss: 0.027454\n",
      "Train Epoch: 71 [32000/45000 (71%)]\tLoss: 0.026909\n",
      "Epoch 71, Val Loss: 0.000340, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 72 [0/45000 (0%)]\tLoss: 0.025049\n",
      "Train Epoch: 72 [32000/45000 (71%)]\tLoss: 0.026237\n",
      "Epoch 72, Val Loss: 0.000337, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 73 [0/45000 (0%)]\tLoss: 0.027321\n",
      "Train Epoch: 73 [32000/45000 (71%)]\tLoss: 0.027334\n",
      "Epoch 73, Val Loss: 0.000337, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 74 [0/45000 (0%)]\tLoss: 0.027720\n",
      "Train Epoch: 74 [32000/45000 (71%)]\tLoss: 0.026187\n",
      "Epoch 74, Val Loss: 0.000336, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 75 [0/45000 (0%)]\tLoss: 0.027801\n",
      "Train Epoch: 75 [32000/45000 (71%)]\tLoss: 0.027771\n",
      "Epoch 75, Val Loss: 0.000336, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 76 [0/45000 (0%)]\tLoss: 0.025496\n",
      "Train Epoch: 76 [32000/45000 (71%)]\tLoss: 0.027677\n",
      "Epoch 76, Val Loss: 0.000335, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 77 [0/45000 (0%)]\tLoss: 0.026871\n",
      "Train Epoch: 77 [32000/45000 (71%)]\tLoss: 0.026841\n",
      "Epoch 77, Val Loss: 0.000334, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 78 [0/45000 (0%)]\tLoss: 0.024860\n",
      "Train Epoch: 78 [32000/45000 (71%)]\tLoss: 0.023692\n",
      "Epoch 78, Val Loss: 0.000334, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 79 [0/45000 (0%)]\tLoss: 0.024959\n",
      "Train Epoch: 79 [32000/45000 (71%)]\tLoss: 0.024697\n",
      "Epoch 79, Val Loss: 0.000332, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 80 [0/45000 (0%)]\tLoss: 0.026700\n",
      "Train Epoch: 80 [32000/45000 (71%)]\tLoss: 0.025305\n",
      "Epoch 80, Val Loss: 0.000329, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 81 [0/45000 (0%)]\tLoss: 0.026239\n",
      "Train Epoch: 81 [32000/45000 (71%)]\tLoss: 0.031101\n",
      "Epoch 81, Val Loss: 0.000330, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 82 [0/45000 (0%)]\tLoss: 0.023597\n",
      "Train Epoch: 82 [32000/45000 (71%)]\tLoss: 0.026734\n",
      "Epoch 82, Val Loss: 0.000331, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 83 [0/45000 (0%)]\tLoss: 0.026821\n",
      "Train Epoch: 83 [32000/45000 (71%)]\tLoss: 0.024011\n",
      "Epoch 83, Val Loss: 0.000329, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 84 [0/45000 (0%)]\tLoss: 0.025688\n",
      "Train Epoch: 84 [32000/45000 (71%)]\tLoss: 0.023574\n",
      "Epoch 84, Val Loss: 0.000327, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 85 [0/45000 (0%)]\tLoss: 0.025484\n",
      "Train Epoch: 85 [32000/45000 (71%)]\tLoss: 0.026586\n",
      "Epoch 85, Val Loss: 0.000327, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 86 [0/45000 (0%)]\tLoss: 0.025518\n",
      "Train Epoch: 86 [32000/45000 (71%)]\tLoss: 0.024190\n",
      "Epoch 86, Val Loss: 0.000327, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 87 [0/45000 (0%)]\tLoss: 0.026466\n",
      "Train Epoch: 87 [32000/45000 (71%)]\tLoss: 0.022922\n",
      "Epoch 87, Val Loss: 0.000325, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 88 [0/45000 (0%)]\tLoss: 0.023188\n",
      "Train Epoch: 88 [32000/45000 (71%)]\tLoss: 0.023890\n",
      "Epoch 88, Val Loss: 0.000324, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 89 [0/45000 (0%)]\tLoss: 0.027606\n",
      "Train Epoch: 89 [32000/45000 (71%)]\tLoss: 0.023326\n",
      "Epoch 89, Val Loss: 0.000325, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 90 [0/45000 (0%)]\tLoss: 0.024128\n",
      "Train Epoch: 90 [32000/45000 (71%)]\tLoss: 0.022203\n",
      "Epoch 90, Val Loss: 0.000322, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 91 [0/45000 (0%)]\tLoss: 0.022587\n",
      "Train Epoch: 91 [32000/45000 (71%)]\tLoss: 0.023348\n",
      "Epoch 91, Val Loss: 0.000322, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 92 [0/45000 (0%)]\tLoss: 0.024208\n",
      "Train Epoch: 92 [32000/45000 (71%)]\tLoss: 0.022923\n",
      "Epoch 92, Val Loss: 0.000322, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 93 [0/45000 (0%)]\tLoss: 0.023803\n",
      "Train Epoch: 93 [32000/45000 (71%)]\tLoss: 0.023828\n",
      "Epoch 93, Val Loss: 0.000318, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 94 [0/45000 (0%)]\tLoss: 0.022566\n",
      "Train Epoch: 94 [32000/45000 (71%)]\tLoss: 0.022805\n",
      "Epoch 94, Val Loss: 0.000317, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 95 [0/45000 (0%)]\tLoss: 0.023707\n",
      "Train Epoch: 95 [32000/45000 (71%)]\tLoss: 0.025400\n",
      "Epoch 95, Val Loss: 0.000320, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 96 [0/45000 (0%)]\tLoss: 0.023364\n",
      "Train Epoch: 96 [32000/45000 (71%)]\tLoss: 0.023329\n",
      "Epoch 96, Val Loss: 0.000319, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 97 [0/45000 (0%)]\tLoss: 0.024238\n",
      "Train Epoch: 97 [32000/45000 (71%)]\tLoss: 0.023397\n",
      "Epoch 97, Val Loss: 0.000321, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 98 [0/45000 (0%)]\tLoss: 0.023820\n",
      "Train Epoch: 98 [32000/45000 (71%)]\tLoss: 0.024352\n",
      "Epoch 98, Val Loss: 0.000319, Accuracy: 5000/5000 (100.00%)\n",
      "Train Epoch: 99 [0/45000 (0%)]\tLoss: 0.023607\n",
      "Train Epoch: 99 [32000/45000 (71%)]\tLoss: 0.024757\n",
      "Epoch 99, Val Loss: 0.000320, Accuracy: 5000/5000 (100.00%)\n",
      "Early stopping at epoch 99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "num_epochs = 200  # Set a high number of epochs\n",
    "\n",
    "#optimizer = optim.Adam(module.parameters(), lr=0.00001, weight_decay=0)\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    module.train()\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        step += 1\n",
    "\n",
    "        #step = batch_idx + len(train_loader)*epoch+1\n",
    "\n",
    "  \n",
    "        # Ensure labels are correctly shaped\n",
    "        labels = labels.view(-1, num_heads)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = module(images, 'training')\n",
    "\n",
    "        # Compute loss for each output head and sum them\n",
    "        total_loss = 0\n",
    "        for i in range(num_heads):\n",
    "            total_loss += criterion(predictions[:, i, :], labels[:, i])\n",
    "        total_loss /= num_heads\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = total_loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(images)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss:.6f}\")\n",
    "\n",
    "    module.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            predictions = module(images, 'inference')\n",
    "            \n",
    "            mimo_output_avg = predictions.mean(dim=1)\n",
    "            val_loss += criterion(mimo_output_avg, labels).item()\n",
    "            _, predicted = torch.max(mimo_output_avg, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Epoch {epoch}, Val Loss: {val_loss:.6f}, Accuracy: {correct}/{len(val_loader.dataset)} ({accuracy:.2f}%)')\n",
    "\n",
    "    '''\n",
    "    wandb.log({\n",
    "        \"Iter step\": step,\n",
    "        \"valid/accs\": accuracy,\n",
    "        #\"test/accs\": test_accs_b,\n",
    "        #\"valid/best_acc\": self.best_valid_acc_b,\n",
    "        #\"test/best_acc\": self.best_test_acc_b,\n",
    "    })\n",
    "    '''\n",
    "\n",
    "    # Check for early stopping based on test loss\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = module.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "module.load_state_dict(best_model_state)\n",
    "\n",
    "#wandb_switch('Multi output module', 'First run', 0, 'finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded model test accuracy: 95.45%\n"
     ]
    }
   ],
   "source": [
    "def test_multioutput_out_last(module, device, test_loader):\n",
    "    module.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            predictions = module(images, 'inference')\n",
    "            mimo_output_avg = predictions[:,-1,:]\n",
    "            _, predicted = torch.max(mimo_output_avg, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_multioutput_out_last(module, device, test_loader)\n",
    "print(f'Reloaded model test accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR10 Train set:\n",
      "Average Entropy for Base Model Outputs: 0.0045\n",
      "Average Entropy for Multi Outputs: 0.0733\n",
      "CIFAR10 Test set:\n",
      "Average Entropy for Base Model Outputs: 0.0461\n",
      "Average Entropy for Multi Outputs: 0.1544\n",
      "FashionMNIST OOD set:\n",
      "Average Entropy for Base Model Outputs: 0.7251\n",
      "Average Entropy for Multi Outputs: 1.1572\n",
      "CIFAR100 OOD set:\n",
      "Average Entropy for Base Model Outputs: 0.4724\n",
      "Average Entropy for Multi Outputs: 0.8114\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to compute and print the mean entropy of softmax outputs for base_model and module\n",
    "def compute_mean_entropy_of_mean_softmax(loader, base_model, module, device):\n",
    "    base_model.eval()\n",
    "    module.eval()\n",
    "\n",
    "    total_base_entropy_sum = 0\n",
    "    total_multi_output_entropy_sum = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in loader:\n",
    "            images = images.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            num_samples += batch_size\n",
    "\n",
    "            # Base model prediction and entropy calculation\n",
    "            base_outputs = base_model(images)\n",
    "            base_output_softmax = F.softmax(base_outputs, dim=1).cpu().numpy()\n",
    "            base_entropies = np.sum(entropy(base_output_softmax, axis=1))\n",
    "            total_base_entropy_sum += base_entropies\n",
    "\n",
    "            # Repeat and reshape images for the multi-output module\n",
    "            multi_output_predictions = module(images, 'inference')\n",
    "            multi_output_avg = multi_output_predictions.mean(dim=1).cpu().numpy()\n",
    "            multi_output_softmax = F.softmax(torch.tensor(multi_output_avg), dim=1).numpy()\n",
    "            multi_output_entropy = np.sum(entropy(multi_output_softmax, axis=1))\n",
    "            total_multi_output_entropy_sum += multi_output_entropy\n",
    "\n",
    "    average_base_entropy = total_base_entropy_sum / num_samples\n",
    "    average_multi_output_entropy = total_multi_output_entropy_sum / num_samples\n",
    "\n",
    "    print(f\"Average Entropy for Base Model Outputs: {average_base_entropy:.4f}\")\n",
    "    print(f\"Average Entropy for Multi Outputs: {average_multi_output_entropy:.4f}\")\n",
    "\n",
    "    return average_base_entropy, average_multi_output_entropy\n",
    "\n",
    "\n",
    "print(\"CIFAR10 Train set:\")\n",
    "train_base_entropy, train_multi_output_entropy = compute_mean_entropy_of_mean_softmax(train_loader, base_model, module, device)\n",
    "print(\"CIFAR10 Test set:\")\n",
    "test_base_entropy, test_multi_output_entropy = compute_mean_entropy_of_mean_softmax(test_loader, base_model, module, device)\n",
    "print(\"FashionMNIST OOD set:\")\n",
    "fashion_base_entropy, fashion_multi_output_entropy = compute_mean_entropy_of_mean_softmax(fashion_ood_loader, base_model, module, device)\n",
    "print(\"CIFAR100 OOD set:\")\n",
    "cifar100_base_entropy, cifar100_multi_output_entropy = compute_mean_entropy_of_mean_softmax(cifar100_ood_loader, base_model, module, device)\n",
    "\n",
    "labels = ['CIFAR10 Train', 'CIFAR10 Test', 'FashionMNIST OOD', 'CIFAR100 OOD']\n",
    "base_entropies = [train_base_entropy, test_base_entropy, fashion_base_entropy, cifar100_base_entropy]\n",
    "multi_output_entropies = [train_multi_output_entropy, test_multi_output_entropy, fashion_multi_output_entropy, cifar100_multi_output_entropy]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, base_entropies, width, label='Base Model')\n",
    "rects2 = ax.bar(x + width/2, multi_output_entropies, width, label='Multi-Output Module')\n",
    "\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_ylabel('Average Entropy of Softmax results')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Accuracy: 96.34%, Brier Score: 0.0061, NLL: 0.1574, ECE: 0.0233, MCE: 0.2711\n",
      "Multi-Output Module Accuracy: 96.29%, Brier Score: 0.0058, NLL: 0.1429, ECE: 0.0179, MCE: 0.2576\n"
     ]
    }
   ],
   "source": [
    "def compute_mce(predictions, confidences, labels, n_bins=15):\n",
    "    \"\"\"\n",
    "    Compute Maximum Calibration Error (MCE).\n",
    "    \n",
    "    Parameters:\n",
    "    - predictions (list or np.array): Model's predicted labels.\n",
    "    - confidences (list or np.array): Model's confidence scores.\n",
    "    - labels (list or np.array): True labels.\n",
    "    - n_bins (int): Number of bins to compute calibration error.\n",
    "    \n",
    "    Returns:\n",
    "    - mce (float): Maximum Calibration Error.\n",
    "    \"\"\"\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    mce = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        if np.any(in_bin):\n",
    "            accuracy_in_bin = np.mean(predictions[in_bin] == labels[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            mce = max(mce, np.abs(avg_confidence_in_bin - accuracy_in_bin))\n",
    "    \n",
    "    return mce\n",
    "\n",
    "def compute_calibration_metrics(loader, base_model, module, device, num_classes=10):\n",
    "    \"\"\"\n",
    "    Compute accuracy and calibration metrics for both base model and multi-output module.\n",
    "    \n",
    "    Parameters:\n",
    "    - loader (DataLoader): Test data loader.\n",
    "    - base_model (nn.Module): Base neural network model.\n",
    "    - module (nn.Module): Multi-output module for calibration.\n",
    "    - device (torch.device): Device to perform computations on.\n",
    "    - num_classes (int): Number of output classes.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing accuracy and calibration metrics for both models.\n",
    "    \"\"\"\n",
    "    base_model.eval()\n",
    "    module.eval()\n",
    "\n",
    "    base_ece_metric = CalibrationError(n_bins=15, task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "    multi_output_ece_metric = CalibrationError(n_bins=15, task=\"multiclass\", num_classes=num_classes).to(device)\n",
    "\n",
    "    base_all_labels, base_all_confidences, base_all_predictions = [], [], []\n",
    "    multi_output_all_labels, multi_output_all_confidences, multi_output_all_predictions = [], [], []\n",
    "\n",
    "    base_correct, base_total, base_test_loss = 0, 0, 0.0\n",
    "    multi_output_correct, multi_output_total, multi_output_test_loss = 0, 0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            base_outputs = base_model(images)\n",
    "            base_test_loss += F.cross_entropy(base_outputs, labels, reduction='sum').item()\n",
    "            base_preds = base_outputs.argmax(dim=1)\n",
    "            base_correct += base_preds.eq(labels).sum().item()\n",
    "            base_total += labels.size(0)\n",
    "\n",
    "            base_softmax_outputs = F.softmax(base_outputs, dim=1)\n",
    "            base_ece_metric.update(base_softmax_outputs, labels)\n",
    "\n",
    "            base_all_labels.extend(labels.cpu().numpy())\n",
    "            base_all_confidences.extend(base_softmax_outputs.cpu().numpy())  # Store full softmax outputs\n",
    "            base_all_predictions.extend(base_preds.cpu().numpy())\n",
    "\n",
    "            multi_output_predictions = module(images, 'inference')\n",
    "            multi_output_softmax = F.softmax(multi_output_predictions, dim=-1)\n",
    "            multi_output_avg = multi_output_softmax.mean(dim=1)\n",
    "            multi_output_test_loss += F.cross_entropy(multi_output_avg, labels, reduction='sum').item()\n",
    "            multi_output_preds = multi_output_avg.argmax(dim=1)\n",
    "            multi_output_correct += multi_output_preds.eq(labels).sum().item()\n",
    "            multi_output_total += labels.size(0)\n",
    "\n",
    "            multi_output_ece_metric.update(multi_output_avg, labels)\n",
    "\n",
    "            multi_output_all_labels.extend(labels.cpu().numpy())\n",
    "            multi_output_all_confidences.extend(multi_output_avg.cpu().numpy())\n",
    "            multi_output_all_predictions.extend(multi_output_preds.cpu().numpy())\n",
    "\n",
    "    base_all_confidences = np.array(base_all_confidences)  # Ensure this is 2D\n",
    "    multi_output_all_confidences = np.array(multi_output_all_confidences)  # Ensure this is 2D\n",
    "\n",
    "    base_accuracy = 100. * base_correct / base_total\n",
    "    base_test_loss /= base_total\n",
    "    base_ece = base_ece_metric.compute().item()\n",
    "\n",
    "    base_nll = log_loss(base_all_labels, base_all_confidences, labels=np.arange(num_classes))\n",
    "    base_brier = mean_squared_error(F.one_hot(torch.tensor(base_all_labels), num_classes=num_classes).numpy(), base_all_confidences)\n",
    "    base_mce = compute_mce(np.array(base_all_predictions), base_all_confidences.max(axis=1), np.array(base_all_labels))\n",
    "\n",
    "    multi_output_accuracy = 100. * multi_output_correct / multi_output_total\n",
    "    multi_output_test_loss /= multi_output_total\n",
    "    multi_output_ece = multi_output_ece_metric.compute().item()\n",
    "\n",
    "    multi_output_nll = log_loss(multi_output_all_labels, multi_output_all_confidences, labels=np.arange(num_classes))\n",
    "    multi_output_brier = mean_squared_error(F.one_hot(torch.tensor(multi_output_all_labels), num_classes=num_classes).numpy(), multi_output_all_confidences)\n",
    "    multi_output_mce = compute_mce(np.array(multi_output_all_predictions), multi_output_all_confidences.max(axis=1), np.array(multi_output_all_labels))\n",
    "\n",
    "    print(f\"Base Model Accuracy: {base_accuracy:.2f}%, Brier Score: {base_brier:.4f}, NLL: {base_nll:.4f}, ECE: {base_ece:.4f}, MCE: {base_mce:.4f}\")\n",
    "    print(f\"Multi-Output Module Accuracy: {multi_output_accuracy:.2f}%, Brier Score: {multi_output_brier:.4f}, NLL: {multi_output_nll:.4f}, ECE: {multi_output_ece:.4f}, MCE: {multi_output_mce:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"Base Model\": {\n",
    "            \"Test NLL\": base_nll,\n",
    "            \"Test Accuracy\": base_accuracy,\n",
    "            \"Test Cal. Error\": base_ece,\n",
    "            \"Test Brier Score\": base_brier,\n",
    "            \"Test MCE\": base_mce,\n",
    "        },\n",
    "        \"Multi-Output Module\": {\n",
    "            \"Test NLL\": multi_output_nll,\n",
    "            \"Test Accuracy\": multi_output_accuracy,\n",
    "            \"Test Cal. Error\": multi_output_ece,\n",
    "            \"Test Brier Score\": multi_output_brier,\n",
    "            \"Test MCE\": multi_output_mce,\n",
    "        }\n",
    "    }\n",
    "\n",
    "accuracy_metrics = compute_calibration_metrics(test_loader, base_model, module, device, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal temperature: 1.377580165863037\n",
      "ECE before temp_scale: 0.02722937172913664\n",
      "ECE after temp_scale: 0.016536326851633615\n"
     ]
    }
   ],
   "source": [
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        \"\"\"\n",
    "        Wrap the base model to apply temperature scaling.\n",
    "        \"\"\"\n",
    "        super(TemperatureScaling, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)  # Initialize temperature to 1.5\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass with temperature scaling applied to logits.\n",
    "        \"\"\"\n",
    "        inputs = inputs.to(next(self.base_model.parameters()).device)\n",
    "        logits = self.base_model(inputs)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        \"\"\"\n",
    "        Scale the logits using the learned temperature.\n",
    "        \"\"\"\n",
    "        return logits / self.temperature.to(logits.device)\n",
    "\n",
    "    def set_temperature(self, validation_loader, device):\n",
    "        \"\"\"\n",
    "        Tune the temperature using validation data by minimizing NLL.\n",
    "        \"\"\"\n",
    "        self.to(device)\n",
    "        self.base_model.eval()\n",
    "        nll_criterion = nn.CrossEntropyLoss().to(device)\n",
    "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            nll_loss = 0.0\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                logits = self.base_model(inputs)\n",
    "                loss = nll_criterion(self.temperature_scale(logits), labels)\n",
    "                nll_loss += loss.item()\n",
    "            nll_loss /= len(validation_loader)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        print(f\"Optimal temperature: {self.temperature.item()}\")\n",
    "\n",
    "def compute_calibration(test_loader, model, device, n_bins=15):\n",
    "    \"\"\"\n",
    "    Compute the Expected Calibration Error (ECE) for a given model and test_loader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    ece = 0.0\n",
    "\n",
    "    bin_boundaries = torch.linspace(0, 1, n_bins + 1).to(device)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    confidences_all = []\n",
    "    predictions_all = []\n",
    "    labels_all = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            confidences, predictions = probabilities.max(dim=1)\n",
    "\n",
    "            confidences_all.append(confidences)\n",
    "            predictions_all.append(predictions)\n",
    "            labels_all.append(labels)\n",
    "\n",
    "    confidences_all = torch.cat(confidences_all)\n",
    "    predictions_all = torch.cat(predictions_all)\n",
    "    labels_all = torch.cat(labels_all)\n",
    "    correct = predictions_all.eq(labels_all)\n",
    "\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences_all > bin_lower) & (confidences_all <= bin_upper)\n",
    "        prop_in_bin = in_bin.float().mean().item()\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = correct[in_bin].float().mean().item()\n",
    "            avg_confidence_in_bin = confidences_all[in_bin].mean().item()\n",
    "            ece += prop_in_bin * abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "\n",
    "    return ece\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "scaled_model = TemperatureScaling(base_model)\n",
    "scaled_model.set_temperature(val_loader, device)\n",
    "\n",
    "ece = compute_calibration(test_loader, base_model, device)\n",
    "print(f'ECE before temp_scale: {ece}')\n",
    "ece = compute_calibration(test_loader, scaled_model, device)\n",
    "print(f'ECE after temp_scale: {ece}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model AUROC: 0.9925\n",
      "Multi-Output Model AUROC: 0.9940\n",
      "Base Model AUR: 0.7948\n",
      "Multi-Output Model AUR: 0.7998\n",
      "Base Model AUROC: 0.9609\n",
      "Multi-Output Model AUROC: 0.9604\n",
      "Base Model AUR: 0.7310\n",
      "Multi-Output Model AUR: 0.7373\n"
     ]
    }
   ],
   "source": [
    "def compute_ood_detection_metrics(train_loader, ood_loader, base_model, module, num_classes, num_heads, device):\n",
    "    base_model.eval()\n",
    "    module.eval()\n",
    "\n",
    "    base_ind_scores = []\n",
    "    base_ood_scores = []\n",
    "    multi_output_ind_scores = []\n",
    "    multi_output_ood_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Process In-Distribution (ID) data\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            base_outputs = base_model(images)\n",
    "            base_output_softmax = F.softmax(base_outputs, dim=1).cpu().numpy()\n",
    "            base_entropy = entropy(base_output_softmax, axis=1)\n",
    "            base_ind_scores.extend(base_entropy)\n",
    "\n",
    "            multi_output_module_output = module(images, 'inference')\n",
    "            multi_output_avg = multi_output_module_output.mean(dim=1)\n",
    "            multi_output_softmax = F.softmax(multi_output_avg, dim=1).cpu().numpy()\n",
    "            multi_output_entropy = entropy(multi_output_softmax, axis=1)\n",
    "            multi_output_ind_scores.extend(multi_output_entropy)\n",
    "\n",
    "        # Process Out-of-Distribution (OOD) data\n",
    "        for images, _ in ood_loader:\n",
    "            images = images.to(device)\n",
    "\n",
    "            base_outputs = base_model(images)\n",
    "            base_output_softmax = F.softmax(base_outputs, dim=1).cpu().numpy()\n",
    "            base_entropy = entropy(base_output_softmax, axis=1)\n",
    "            base_ood_scores.extend(base_entropy)\n",
    "\n",
    "            multi_output_module_output = module(images, 'inference')\n",
    "            multi_output_avg = multi_output_module_output.mean(dim=1)\n",
    "            multi_output_softmax = F.softmax(multi_output_avg, dim=1).cpu().numpy()\n",
    "            multi_output_entropy = entropy(multi_output_softmax, axis=1)\n",
    "            multi_output_ood_scores.extend(multi_output_entropy)\n",
    "\n",
    "    # Generating labels (0 for in-distribution, 1 for OOD)\n",
    "    base_labels = np.concatenate([np.zeros(len(base_ind_scores)), np.ones(len(base_ood_scores))])\n",
    "    base_scores = np.concatenate([base_ind_scores, base_ood_scores])\n",
    "    base_auroc = roc_auc_score(base_labels, base_scores)\n",
    "\n",
    "    multi_output_labels = np.concatenate([np.zeros(len(multi_output_ind_scores)), np.ones(len(multi_output_ood_scores))])\n",
    "    multi_output_scores = np.concatenate([multi_output_ind_scores, multi_output_ood_scores])\n",
    "    multi_output_auroc = roc_auc_score(multi_output_labels, multi_output_scores)\n",
    "\n",
    "    # Compute Area Under Recall (AUR)\n",
    "    base_precision, base_recall, _ = precision_recall_curve(base_labels, base_scores)\n",
    "    base_aur = np.trapz(base_recall, base_precision)  # AUR for base model\n",
    "\n",
    "    multi_output_precision, multi_output_recall, _ = precision_recall_curve(multi_output_labels, multi_output_scores)\n",
    "    multi_output_aur = np.trapz(multi_output_recall, multi_output_precision)  # AUR for multi-output model\n",
    "\n",
    "    print(f\"Base Model AUROC: {base_auroc:.4f}\")\n",
    "    print(f\"Multi-Output Model AUROC: {multi_output_auroc:.4f}\")\n",
    "    print(f\"Base Model AUR: {base_aur:.4f}\")\n",
    "    print(f\"Multi-Output Model AUR: {multi_output_aur:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"base_auroc\": base_auroc,\n",
    "        \"multi_output_auroc\": multi_output_auroc,\n",
    "        \"base_aur\": base_aur,\n",
    "        \"multi_output_aur\": multi_output_aur,\n",
    "    }\n",
    "\n",
    "fashion_ood_metrics = compute_ood_detection_metrics(train_loader, fashion_ood_loader, base_model, module, num_classes, num_heads, device)\n",
    "cifar100_ood_metrics = compute_ood_detection_metrics(train_loader, cifar100_ood_loader, base_model, module, num_classes, num_heads, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
